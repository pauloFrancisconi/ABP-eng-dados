{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Documenta\u00e7\u00e3o do Projeto de Engenharia de Dados","text":"<p>Este projeto consiste em uma arquitetura de engenharia de dados na nuvem utilizando os principais servi\u00e7os da Azure. O fluxo contempla ingest\u00e3o, armazenamento, transforma\u00e7\u00e3o e visualiza\u00e7\u00e3o de dados.</p>"},{"location":"#ferramentas-utilizadas","title":"Ferramentas utilizadas","text":"<ul> <li>Banco de dados: Azure SQL Database</li> <li>Orquestra\u00e7\u00e3o: Azure Data Factory</li> <li>Processamento: Azure Databricks</li> <li>Armazenamento: Azure Data Lake Storage Gen2</li> <li>Visualiza\u00e7\u00e3o: Power BI</li> </ul>"},{"location":"#estrutura-do-repositorio","title":"Estrutura do Reposit\u00f3rio","text":"<ul> <li><code>data/</code>: Scripts SQL para cria\u00e7\u00e3o do banco de dados relacional, schema e tabelas. Diagramas ER tamb\u00e9m podem ser adicionados a partir de <code>/assets</code>.</li> <li><code>Iac/</code>: Infraestrutura como C\u00f3digo (IaC) com Terraform para provisionamento dos recursos na Azure.</li> <li>Subpastas para cada servi\u00e7o: <code>adls/</code>, <code>sql_server/</code>, <code>az_databricks/</code>, <code>adf/</code>, <code>resource_group/</code>.</li> <li><code>assets/</code>: Imagens, diagramas ER e arquivos auxiliares.</li> <li><code>docs/</code>: Documenta\u00e7\u00e3o completa da aplica\u00e7\u00e3o (este diret\u00f3rio).</li> <li><code>src/</code>: Pasta com subpastas <code>faker/</code>: Script Python que gera dados fict\u00edcios e insere no banco SQL e <code>notebooks/</code>: Notebooks Python com os scripts de transforma\u00e7\u00e3o de arquivos no ADLS Gen2 utilizando Azure Databricks.</li> </ul>"},{"location":"#conteudo-da-documentacao","title":"Conte\u00fado da Documenta\u00e7\u00e3o","text":"<ul> <li>Vis\u00e3o Geral da Arquitetura</li> <li>Provisionamento com Terraform (IaC)</li> <li>Banco de Dados e Diagramas ER</li> <li>Cria\u00e7\u00e3o de Dados com Faker</li> <li>Notebooks do Databricks (ETL)</li> <li>Azure Databricks</li> <li>Pipelines no Azure Data Factory - Configura\u00e7\u00e3o</li> <li>Pipelines no Azure Data Factory - Pipeline</li> <li>Dashboard no Power BI</li> </ul>"},{"location":"adf-config/","title":"Azure Data Factory - Integra\u00e7\u00e3o de Dados","text":"<p>Este documento explica como acessar o Azure Data Factory criado via Terraform, configurar os servi\u00e7os vinculados (Linked Services) e criar os conjuntos de dados (Datasets) necess\u00e1rios para a movimenta\u00e7\u00e3o de dados entre o Azure SQL Database, Azure Data Lake Storage Gen2 e o Azure Databricks.</p>"},{"location":"adf-config/#1-acessando-o-azure-data-factory","title":"1. Acessando o Azure Data Factory","text":"<p>Acesse o recurso Azure Data Factory criado por Terraform em seu ambiente Azure:</p> <ol> <li>V\u00e1 para o Portal Azure</li> <li>Na barra de pesquisa, digite \"Data Factory\"</li> <li>Selecione o recurso com o nome provisionado no Terraform.</li> <li>Clique em \"Iniciar o Studio\" (Open Azure Data Factory Studio) para acessar o ambiente de integra\u00e7\u00e3o visual.</li> </ol>"},{"location":"adf-config/#2-configurando-os-linked-services","title":"2. Configurando os Linked Services","text":"<p>Com o Azure Data Factory aberto:</p> <ol> <li>No menu \u00e0 esquerda, clique em \"Gerenciar\" (Manage).</li> <li>Selecione \"Servi\u00e7os vinculados\" (Linked Services).</li> <li>Clique em \"Novo\" para adicionar os servi\u00e7os abaixo:</li> </ol>"},{"location":"adf-config/#a-azure-sql-database","title":"a) Azure SQL Database","text":"<ul> <li>Tipo: Azure SQL Database</li> <li>Inscri\u00e7\u00e3o: Selecione sua subscri\u00e7\u00e3o Azure</li> <li>Banco de dados: Selecione o provisionado via Terraform</li> <li>Autentica\u00e7\u00e3o: Nome de usu\u00e1rio e senha (Admin do SQL Server)</li> </ul>"},{"location":"adf-config/#b-azure-data-lake-storage-gen2","title":"b) Azure Data Lake Storage Gen2","text":"<ul> <li>Tipo: Azure Data Lake Storage Gen2</li> <li>Inscri\u00e7\u00e3o: Selecione sua subscri\u00e7\u00e3o Azure</li> <li>Conta de Armazenamento: Selecione a provisionada no Terraform</li> <li>Autentica\u00e7\u00e3o: Conta de Servi\u00e7o (Managed Identity ou Chave de Conta)</li> </ul>"},{"location":"adf-config/#c-azure-databricks","title":"c) Azure Databricks","text":"<ul> <li>Tipo: Azure Databricks</li> <li>Inscri\u00e7\u00e3o: Selecione sua subscri\u00e7\u00e3o Azure</li> <li>Workspace: Selecione o workspace Databricks criado via Terraform</li> <li>Autentica\u00e7\u00e3o: Token pessoal do Databricks  </li> <li>Caso n\u00e3o tenha um token, consulte a Documenta\u00e7\u00e3o do Azure Databricks</li> <li>Cluster: Selecione o cluster interativo existente (criado anteriormente)</li> </ul>"},{"location":"adf-config/#3-criando-os-conjuntos-de-dados-datasets","title":"3. Criando os Conjuntos de Dados (Datasets)","text":""},{"location":"adf-config/#a-dataset-de-origem-azure-sql-database","title":"a) Dataset de Origem: Azure SQL Database","text":"<p>Crie um novo dataset para conectar-se \u00e0s tabelas do banco de dados SQL hospedado no Azure:</p> <ul> <li>Tipo: Azure SQL Database</li> <li>Linked Service: Selecione o servi\u00e7o criado acima</li> <li>Query/Table: Selecione a tabela desejada ou use par\u00e2metros</li> </ul> <p>Exemplo de par\u00e2metro configurado:</p> <p></p> <p>JSON do Dataset:</p> <p>JSON</p> <pre><code>{\n    \"name\": \"DS_SQL_Parametro\",\n    \"properties\": {\n        \"linkedServiceName\": {\n            \"referenceName\": \"AzureSqlDatabase\",\n            \"type\": \"LinkedServiceReference\"\n        },\n        \"parameters\": {\n            \"tabela_nome\": {\n                \"type\": \"string\"\n            }\n        },\n        \"annotations\": [],\n        \"type\": \"AzureSqlTable\",\n        \"schema\": [],\n        \"typeProperties\": {\n            \"schema\": \"relacional\",\n            \"table\": \"avaliacoes\"\n        }\n    }\n}\n</code></pre>"},{"location":"adf-config/#b-dataset-de-destino-azure-data-lake-storage-gen2-delimited-text","title":"b) Dataset de Destino: Azure Data Lake Storage Gen2 (Delimited Text)","text":"<p>Configure um dataset para exportar os dados em formato <code>.csv</code> para a landing zone no Data Lake:</p> <ul> <li>Tipo: Delimited Text</li> <li>Linked Service: Azure Data Lake Storage Gen2</li> <li>Pasta de destino: landing-zone/</li> <li>Nome do arquivo: pode ser parametrizado (ex: <code>@dataset().nome_arquivo</code>)</li> <li>Delimitador: <code>,</code></li> <li>First Row as Header: true</li> </ul> <p>Exemplo de configura\u00e7\u00e3o:</p> <p></p> <p>JSON do Dataset:</p> <p>JSON</p> <pre><code>{\n    \"name\": \"DS_DEST_CSV\",\n    \"properties\": {\n        \"linkedServiceName\": {\n            \"referenceName\": \"AzureDataLakeStorage\",\n            \"type\": \"LinkedServiceReference\"\n        },\n        \"parameters\": {\n            \"nome_arquivo\": {\n                \"type\": \"string\"\n            }\n        },\n        \"annotations\": [],\n        \"type\": \"DelimitedText\",\n        \"typeProperties\": {\n            \"location\": {\n                \"type\": \"AzureBlobFSLocation\",\n                \"fileSystem\": {\n                    \"value\": \"@concat(\\n  'landing-zone/ecommerce/',\\n  trim(\\n    uriComponent(\\n      trim(\\n        split(trim(dataset().nome_arquivo), '.')[1]\\n      )\\n    )\\n  ),\\n  '.csv'\\n)\",\n                    \"type\": \"Expression\"\n                }\n            },\n            \"columnDelimiter\": \",\",\n            \"escapeChar\": \"\\\\\",\n            \"firstRowAsHeader\": true,\n            \"quoteChar\": \"\\\"\"\n        },\n        \"schema\": []\n    }\n}\n</code></pre> <p>\u26a0\ufe0f Importante: Certifique-se de testar a conex\u00e3o de cada Linked Service e Dataset ap\u00f3s a cria\u00e7\u00e3o.</p> <p>Ir para cira\u00e7\u00e3o da pipeline no Azure Data Factory</p>"},{"location":"adf-pipeline/","title":"Azure Data Factory - Pipeline de Integra\u00e7\u00e3o","text":"<p>Este documento descreve como criar e configurar uma pipeline no Azure Data Factory para orquestrar a movimenta\u00e7\u00e3o e transforma\u00e7\u00e3o de dados entre o Azure SQL Database, Azure Data Lake Storage Gen2 e notebooks do Azure Databricks.</p>"},{"location":"adf-pipeline/#1-set-variable-definindo-as-tabelas","title":"1. Set Variable - Definindo as Tabelas","text":"<p>A primeira atividade da pipeline \u00e9 definir uma vari\u00e1vel do tipo array com os nomes das tabelas que ser\u00e3o copiadas do banco de dados.</p> <ol> <li>Na aba de atividades, arraste a atividade Set Variable para a pipeline.</li> <li>Crie uma nova vari\u00e1vel do tipo Array, por exemplo: <code>tabelas</code>.</li> <li>Configure o valor com os nomes das tabelas, por exemplo:</li> </ol> <p>\ud83d\udcf7 Exemplo de configura\u00e7\u00e3o:</p> <p></p> <p>\ud83d\udcc4 Configura\u00e7\u00e3o do array:</p> <p>@{   \"tabelas\": [     \"clientes\",     \"vendedores\",     \"produtos\",     \"pedidos\",     \"pagamentos\"   ] }</p>"},{"location":"adf-pipeline/#2-foreach-loop-nas-tabelas","title":"2. Foreach - Loop nas Tabelas","text":"<ol> <li>Adicione a atividade Foreach ap\u00f3s o <code>Set Variable</code>.</li> <li>Na aba \"Configura\u00e7\u00f5es\" do Foreach, defina como item sequencial e adicione a seguinte express\u00e3o no campo de itens:</li> </ol> <p>@<code>@variables('tabelas')</code></p> <ol> <li>Dentro do Foreach, adicione uma atividade Copy Data.</li> </ol>"},{"location":"adf-pipeline/#configuracoes-da-atividade-copy-data","title":"Configura\u00e7\u00f5es da Atividade Copy Data:","text":"<ul> <li>Origem: Dataset do Azure SQL Database com suporte a par\u00e2metro de nome de tabela.</li> <li>Destino: Dataset de arquivo delimitado no ADLS (tamb\u00e9m parametrizado).</li> </ul> <p>\ud83d\udcf7 Exemplo de configura\u00e7\u00e3o:</p> <p></p> <p>\ud83d\udcc4 Exemplo de mapeamento din\u00e2mico:</p> <p>@{   \"source\": {     \"type\": \"AzureSqlSource\",     \"sqlReaderQuery\": \"SELECT * FROM @{item()}\"   },   \"sink\": {     \"type\": \"DelimitedTextSink\"   },   \"translator\": {     \"type\": \"TabularTranslator\",     \"typeConversion\": true,     \"typeConversionMode\": \"AllowCompatible\"   } }</p>"},{"location":"adf-pipeline/#3-executando-notebook-bronze-no-databricks","title":"3. Executando Notebook Bronze no Databricks","text":"<ol> <li>Ap\u00f3s o loop, adicione uma nova atividade: Databricks Notebook.</li> <li>Configure com o servi\u00e7o vinculado (Linked Service) do Databricks criado anteriormente.</li> <li>Preencha o campo Path com o caminho do notebook bronze, que deve estar no workspace.</li> </ol> <p>\u2139\ufe0f O caminho do notebook pode ser encontrado conforme instru\u00eddo na documenta\u00e7\u00e3o do Azure Databricks.</p>"},{"location":"adf-pipeline/#4-executando-notebook-silver-no-databricks","title":"4. Executando Notebook Silver no Databricks","text":"<ol> <li>Adicione outra atividade Databricks Notebook.</li> <li>Use o mesmo Linked Service.</li> <li>Preencha com o caminho do notebook silver.</li> </ol>"},{"location":"adf-pipeline/#5-executando-notebook-gold-no-databricks","title":"5. Executando Notebook Gold no Databricks","text":"<ol> <li>Adicione mais uma atividade Databricks Notebook.</li> <li>Configure novamente com o Linked Service e o caminho do notebook gold.</li> </ol>"},{"location":"adf-pipeline/#6-consideracoes-finais","title":"6. Considera\u00e7\u00f5es Finais","text":"<ul> <li>Certifique-se de que o cluster do Databricks est\u00e1 ativo antes da execu\u00e7\u00e3o dos notebooks.</li> <li>O nome das tabelas no array do <code>Set Variable</code> deve existir no banco SQL.</li> <li>Os datasets devem estar parametrizados para receber dinamicamente o nome da tabela e do arquivo.</li> <li>O pipeline pode ser agendado ou executado manualmente.</li> <li>Todas as depend\u00eancias (Linked Services, Datasets, e Notebooks) devem estar previamente criadas.</li> </ul>"},{"location":"arquitetura/","title":"Arquitetura do Projeto","text":""},{"location":"arquitetura/#visao-geral","title":"Vis\u00e3o Geral","text":"<p>Este projeto utiliza uma arquitetura moderna baseada em servi\u00e7os gerenciados da Azure para ingest\u00e3o, processamento, armazenamento e visualiza\u00e7\u00e3o de dados.</p>"},{"location":"arquitetura/#componentes-principais","title":"Componentes Principais","text":"<ul> <li>Azure SQL Database: Armazena dados transacionais.</li> <li>Azure Data Lake Storage Gen2 (ADLS): Armazena dados em camadas (Landing, Bronze, Silver, Gold).</li> <li>Azure Data Factory (ADF): Orquestra os pipelines de ingest\u00e3o e transforma\u00e7\u00e3o.</li> <li>Azure Databricks: Processa os dados em notebooks (ETL).</li> <li>Power BI: Conecta-se ao Silver ou Gold para cria\u00e7\u00e3o de dashboards.</li> </ul>"},{"location":"arquitetura/#diagrama-da-arquitetura","title":"Diagrama da Arquitetura","text":""},{"location":"azdabri/","title":"Utilizando o Azure Databricks","text":"<p>Este guia descreve os passos necess\u00e1rios para acessar o workspace do Azure Databricks criado via Terraform, importar notebooks, configurar um cluster de computa\u00e7\u00e3o e gerar um token de acesso.</p>"},{"location":"azdabri/#1-acessando-o-workspace-e-importando-notebooks","title":"1. Acessando o Workspace e Importando Notebooks","text":"<p>Ap\u00f3s a execu\u00e7\u00e3o do Terraform, o recurso Azure Databricks estar\u00e1 provisionado. Caso voc\u00ea n\u00e3o tenha feito o provisionamento dos resursos, acesse a documenta\u00e7\u00e3o do Terraform do projeto. Para acessar o workspace:</p> <ol> <li>Acesse o portal do Azure.</li> <li>Pesquise por \"Azure Databricks\" e clique no recurso criado.</li> <li>Clique em \"Launch Workspace\" para abrir a interface do Databricks.</li> <li>No menu lateral esquerdo, v\u00e1 at\u00e9 a se\u00e7\u00e3o \"Workspace\".</li> <li>Clique com o bot\u00e3o direito em seu usu\u00e1rio ou pasta principal e selecione \"Create &gt; Folder\".</li> <li> <p>Nomeie a pasta como <code>notebooks</code>.</p> </li> <li> <p>Dentro da pasta <code>notebooks</code>, clique em \"Import\".</p> </li> <li>Selecione os arquivos localizados na pasta do projeto: <code>src/notebooks/</code></li> <li>Importe todos os arquivos <code>.ipynb</code> necess\u00e1rios para o seu trabalho.</li> </ol>"},{"location":"azdabri/#2-criando-um-cluster-de-computacao","title":"2. Criando um Cluster de Computa\u00e7\u00e3o","text":"<p>Para executar os notebooks, ser\u00e1 necess\u00e1rio configurar um cluster Databricks.</p> <ol> <li>No menu esquerdo, clique em \"Compute\".</li> <li>Clique no bot\u00e3o \"Create Cluster\".</li> <li> <p>Preencha as informa\u00e7\u00f5es conforme abaixo:</p> </li> <li> <p>Cluster name: Nome de sua escolha (ex: <code>cluster-projeto</code>)</p> </li> <li>Databricks Runtime Version: <code>10.4 LTS (Scala 2.12, Spark 3.2.1)</code></li> <li>Node type: <code>Standard_D4s_v3</code></li> <li> <p>Autopilot Options: Pode deixar as op\u00e7\u00f5es padr\u00e3o ou configurar conforme necessidade.</p> </li> <li> <p>Clique em \"Create Cluster\".</p> </li> </ol> <p></p>"},{"location":"azdabri/#3-gerando-um-token-de-acesso-para-o-azure-data-factory","title":"3. Gerando um Token de Acesso (para o Azure Data Factory)","text":"<p>Para integra\u00e7\u00e3o com outros servi\u00e7os, como o Azure Data Factory, \u00e9 necess\u00e1rio um token de acesso do Databricks.</p> <ol> <li>No canto superior direito da interface do Databricks, clique no \u00edcone de usu\u00e1rio e depois em \"User Settings\".</li> <li>V\u00e1 at\u00e9 a aba \"Developer\" ou \"Access Tokens\".</li> <li>Clique em \"Generate New Token\".</li> <li>Defina o tempo de expira\u00e7\u00e3o desejado.</li> <li>Clique em \"Generate\" e copie o token gerado.</li> </ol> <p>\u26a0\ufe0f Aten\u00e7\u00e3o: o token n\u00e3o poder\u00e1 ser recuperado novamente, portanto salve-o com seguran\u00e7a. Voc\u00ea ir\u00e1 utiliz\u00e1-lo posteriormente na configura\u00e7\u00e3o do Azure Data Factory.</p>"},{"location":"azdabri/#recursos-relacionados","title":"Recursos Relacionados","text":"<ul> <li>Pasta com notebooks: <code>src/notebooks/</code></li> <li>Imagem de refer\u00eancia do cluster: <code>assets/foto-compute.png</code></li> </ul>"},{"location":"azdabri/#requisitos","title":"Requisitos","text":"<ul> <li>O Azure Databricks deve estar provisionado via Terraform.</li> <li>Voc\u00ea precisa de permiss\u00f5es para acessar o recurso e criar notebooks/cluster.</li> </ul>"},{"location":"database/","title":"Documenta\u00e7\u00e3o do Banco de Dados - Projeto E-commerce","text":""},{"location":"database/#visao-geral","title":"Vis\u00e3o Geral","text":"<p>Este projeto utiliza o Azure SQL Server para gerenciar os dados de um sistema de e-commerce. O banco de dados foi estruturado com base em um modelo relacional, contendo entidades que representam clientes, pedidos, produtos, pagamentos, entregas, entre outros.</p> <p>O passo a passo completo para cria\u00e7\u00e3o da inst\u00e2ncia no Azure est\u00e1 descrito aqui.</p>"},{"location":"database/#modelo-fisico","title":"Modelo F\u00edsico","text":"<p>O diagrama abaixo representa o modelo relacional f\u00edsico utilizado no banco de dados, com as tabelas, colunas e relacionamentos:</p> <p></p>"},{"location":"database/#modelo-dimensional","title":"Modelo Dimensional","text":"<p>O modelo dimensional \u00e9 utilizado para fins anal\u00edticos e gera\u00e7\u00e3o de relat\u00f3rios, organizando os dados em fatos e dimens\u00f5es.</p> <p></p>"},{"location":"database/#descricao-das-tabelas","title":"Descri\u00e7\u00e3o das Tabelas","text":""},{"location":"database/#clientes","title":"<code>clientes</code>","text":"<p>Armazena os dados dos clientes cadastrados.</p> Coluna Tipo Descri\u00e7\u00e3o <code>id</code> int (PK) Identificador \u00fanico <code>nome</code> varchar(100) Nome completo <code>email</code> varchar(150) E-mail \u00fanico <code>telefone</code> varchar(20) Telefone de contato <code>data_cadastro</code> datetime Data de cadastro"},{"location":"database/#vendedores","title":"<code>vendedores</code>","text":"<p>Informa\u00e7\u00f5es dos vendedores da plataforma.</p> Coluna Tipo Descri\u00e7\u00e3o <code>id</code> int (PK) Identificador \u00fanico <code>nome</code> varchar(100) Nome do vendedor <code>email</code> varchar(150) E-mail \u00fanico <code>telefone</code> varchar(20) Telefone de contato <code>data_cadastro</code> datetime Data de cadastro"},{"location":"database/#enderecos_cliente","title":"<code>enderecos_cliente</code>","text":"<p>Lista de endere\u00e7os vinculados a clientes.</p> Coluna Tipo Descri\u00e7\u00e3o <code>id</code> int (PK) Identificador do endere\u00e7o <code>cliente_id</code> int (FK) Cliente associado <code>logradouro</code> varchar(150) Nome da rua <code>numero</code> varchar(10) N\u00famero da resid\u00eancia <code>complemento</code> varchar(50) Complemento do endere\u00e7o <code>bairro</code> varchar(50) Bairro <code>cidade</code> varchar(50) Cidade <code>estado</code> varchar(2) UF <code>cep</code> varchar(10) CEP"},{"location":"database/#categorias","title":"<code>categorias</code>","text":"<p>Categorias de produtos.</p> Coluna Tipo Descri\u00e7\u00e3o <code>id</code> int (PK) Identificador <code>nome</code> varchar(50) Nome da categoria <code>descricao</code> varchar(150) Descri\u00e7\u00e3o"},{"location":"database/#produtos","title":"<code>produtos</code>","text":"<p>Produtos cadastrados no sistema.</p> Coluna Tipo Descri\u00e7\u00e3o <code>id</code> int (PK) Identificador <code>vendedor_id</code> int (FK) Vendedor respons\u00e1vel <code>categoria_id</code> int (FK) Categoria do produto <code>nome</code> varchar(100) Nome <code>descricao</code> varchar(MAX) Descri\u00e7\u00e3o detalhada <code>preco</code> decimal(10,2) Pre\u00e7o <code>data_cadastro</code> datetime Data de cadastro"},{"location":"database/#estoque","title":"<code>estoque</code>","text":"<p>Controle de quantidade em estoque.</p> Coluna Tipo Descri\u00e7\u00e3o <code>id</code> int (PK) Identificador <code>produto_id</code> int (FK) Produto relacionado <code>quantidade</code> int Quantidade dispon\u00edvel"},{"location":"database/#pedidos","title":"<code>pedidos</code>","text":"<p>Pedidos realizados pelos clientes.</p> Coluna Tipo Descri\u00e7\u00e3o <code>id</code> int (PK) Identificador do pedido <code>cliente_id</code> int (FK) Cliente que realizou o pedido <code>endereco_entrega_id</code> int (FK) Endere\u00e7o de entrega <code>data_pedido</code> datetime Data do pedido <code>status</code> varchar(50) Status do pedido <code>total</code> decimal(10,2) Valor total do pedido"},{"location":"database/#itens_pedido","title":"<code>itens_pedido</code>","text":"<p>Itens que comp\u00f5em um pedido.</p> Coluna Tipo Descri\u00e7\u00e3o <code>id</code> int (PK) Identificador <code>pedido_id</code> int (FK) Pedido associado <code>produto_id</code> int (FK) Produto adquirido <code>quantidade</code> int Quantidade comprada <code>preco_unitario</code> decimal(10,2) Valor unit\u00e1rio na compra"},{"location":"database/#formas_pagamento","title":"<code>formas_pagamento</code>","text":"<p>Formas aceitas para pagamento.</p> Coluna Tipo Descri\u00e7\u00e3o <code>id</code> int (PK) Identificador <code>descricao</code> varchar(50) Descri\u00e7\u00e3o (Cart\u00e3o, PIX, Boleto etc.)"},{"location":"database/#pagamentos","title":"<code>pagamentos</code>","text":"<p>Pagamentos efetuados nos pedidos.</p> Coluna Tipo Descri\u00e7\u00e3o <code>id</code> int (PK) Identificador do pagamento <code>pedido_id</code> int (FK) Pedido pago <code>forma_pagamento_id</code> int (FK) Forma de pagamento utilizada <code>valor_pago</code> decimal(10,2) Valor efetivamente pago <code>data_pagamento</code> datetime Data do pagamento <code>status</code> varchar(50) Status (Aprovado, Recusado, etc.)"},{"location":"database/#transportadoras","title":"<code>transportadoras</code>","text":"<p>Empresas respons\u00e1veis pelas entregas.</p> Coluna Tipo Descri\u00e7\u00e3o <code>id</code> int (PK) Identificador <code>nome</code> varchar(100) Nome da transportadora <code>telefone</code> varchar(20) Contato <code>email</code> varchar(150) E-mail"},{"location":"database/#entregas","title":"<code>entregas</code>","text":"<p>Informa\u00e7\u00f5es sobre o envio dos pedidos.</p> Coluna Tipo Descri\u00e7\u00e3o <code>id</code> int (PK) Identificador <code>pedido_id</code> int (FK) Pedido entregue <code>transportadora_id</code> int (FK) Transportadora respons\u00e1vel <code>data_envio</code> datetime Data de envio <code>data_entrega</code> datetime Data de entrega <code>status</code> varchar(50) Status da entrega"},{"location":"database/#avaliacoes","title":"<code>avaliacoes</code>","text":"<p>Avalia\u00e7\u00f5es feitas por clientes nos produtos.</p> Coluna Tipo Descri\u00e7\u00e3o <code>id</code> int (PK) Identificador <code>cliente_id</code> int (FK) Cliente que avaliou <code>produto_id</code> int (FK) Produto avaliado <code>nota</code> int Nota de avalia\u00e7\u00e3o (1 a 5, por ex.) <code>comentario</code> varchar(MAX) Coment\u00e1rio textual <code>data_avaliacao</code> datetime Data da avalia\u00e7\u00e3o"},{"location":"faker/","title":"Gerando Dados Falsos com Faker","text":"<p>Nesta etapa, utilizamos a biblioteca Faker para popular automaticamente o banco de dados com dados sint\u00e9ticos realistas. Isso facilita o teste do sistema com um volume significativo de dados.</p>"},{"location":"faker/#arquivos-relacionados","title":"Arquivos Relacionados","text":"<ul> <li><code>faker_data.py</code> \u2014 Script principal de gera\u00e7\u00e3o e inser\u00e7\u00e3o de dados.</li> <li><code>.env.example</code> \u2014 Modelo de vari\u00e1veis de ambiente necess\u00e1rias.</li> <li><code>teste_conexao.py</code> \u2014 Script para testar a conex\u00e3o com o banco.</li> </ul>"},{"location":"faker/#como-funciona","title":"Como Funciona","text":"<p>O script <code>faker_data.py</code> utiliza:</p> <ul> <li>Faker para gerar dados como nomes, emails, endere\u00e7os, datas etc.</li> <li>pyodbc para conectar-se ao SQL Server hospedado na Azure.</li> <li>dotenv para carregar vari\u00e1veis de ambiente de um arquivo <code>.env</code>.</li> </ul> <p>Ele gera dados para diversas tabelas do schema <code>relacional</code>, como:</p> <ul> <li><code>clientes</code>, <code>vendedores</code>, <code>produtos</code>, <code>pedidos</code>, <code>pagamentos</code>, <code>entregas</code> e mais.</li> <li>Categorias e formas de pagamento s\u00e3o fixas.</li> <li>A inser\u00e7\u00e3o \u00e9 feita em lote, e a execu\u00e7\u00e3o s\u00f3 come\u00e7a se todas as tabelas existirem.</li> </ul>"},{"location":"faker/#antes-de-executar","title":"Antes de Executar","text":""},{"location":"faker/#1-criar-o-banco-de-dados-se-necessario","title":"1. Criar o Banco de Dados (se necess\u00e1rio)","text":"<p>Se ainda n\u00e3o houver um Azure SQL Database criado, consulte a documenta\u00e7\u00e3o de provisionamento com Terraform:</p> <p>Provisionamento com Terraform</p>"},{"location":"faker/#2-instalar-o-driver-odbc-para-sql-server","title":"2. Instalar o Driver ODBC para SQL Server","text":"<p>\u00c9 necess\u00e1rio ter o driver ODBC instalado no sistema.</p> <ul> <li> <p>Windows: Download do ODBC Driver</p> </li> <li> <p>Linux/Mac:   Consulte o guia oficial para instru\u00e7\u00f5es espec\u00edficas.</p> </li> </ul>"},{"location":"faker/#3-configurar-as-variaveis-de-ambiente","title":"3. Configurar as Vari\u00e1veis de Ambiente","text":"<p>Crie um arquivo <code>.env</code> na raiz do projeto com base no <code>.env.example</code>:</p> <pre><code>DB_SERVER=seu_servidor.database.windows.net  \nDB_DATABASE=nome_do_banco  \nDB_USERNAME=seu_usuario  \nDB_PASSWORD=sua_senha  \nDB_DRIVER=ODBC Driver 18 for SQL Server  \n</code></pre>"},{"location":"faker/#4-garantir-que-as-tabelas-estao-criadas","title":"4. Garantir que as Tabelas Est\u00e3o Criadas","text":"<p>Antes de rodar o script, certifique-se de que todas as tabelas do banco j\u00e1 foram criadas.</p> <p>Consulte:</p> <ul> <li>Scripts SQL fornecidos pelo projeto, ou  </li> <li>A documenta\u00e7\u00e3o de banco de dados: Documenta\u00e7\u00e3o do Banco</li> </ul>"},{"location":"faker/#5-testar-a-conexao-com-o-banco","title":"5. Testar a Conex\u00e3o com o Banco","text":"<p>Execute o script de teste:</p> <pre><code>python teste_conexao.py  \n</code></pre>"},{"location":"faker/#6-executar-o-script-faker","title":"6. Executar o Script Faker","text":"<p>Se a conex\u00e3o estiver correta, execute:</p> <pre><code>python faker_data.py  \n</code></pre>"},{"location":"faker/#observacoes-importantes","title":"\u26a0\ufe0f Observa\u00e7\u00f5es Importantes","text":"<ul> <li>O script valida a exist\u00eancia das tabelas antes de iniciar.</li> <li>Dados s\u00e3o sempre os mesmos, pois o script usa seeds fixas (<code>Faker.seed(42)</code>, <code>random.seed(42)</code>).</li> <li>N\u00e3o h\u00e1 limpeza de dados anterior \u2014 evite rodar m\u00faltiplas vezes se o banco j\u00e1 contiver registros.</li> <li>Se alguma tabela estiver faltando, a execu\u00e7\u00e3o ser\u00e1 interrompida com mensagem de erro.</li> <li>Para garantir que tudo est\u00e1 em ordem, verifique a estrutura do banco de dados antes de usar o Faker.</li> </ul>"},{"location":"iac/","title":"Infraestrutura como C\u00f3digo (IaC) com Terraform","text":"<p>Este projeto utiliza Terraform para provisionar a infraestrutura na Azure, adotando uma abordagem modular. Cada m\u00f3dulo est\u00e1 isolado em sua pr\u00f3pria pasta e deve ser aplicado separadamente.</p>"},{"location":"iac/#estrutura-dos-modulos","title":"Estrutura dos M\u00f3dulos","text":"<p>Cada pasta representa um m\u00f3dulo de infraestrutura:</p> <ul> <li><code>resource_group/</code> \u2014 Grupo de recursos no Azure  </li> <li><code>sql_server/</code> \u2014 Azure SQL Database</li> <li><code>adls/</code> \u2014 Azure Data Lake Storage  </li> <li><code>az_databricks/</code> \u2014 Azure Databricks  </li> <li><code>adf/</code> \u2014 Azure Data Factory  </li> </ul>"},{"location":"iac/#como-executar","title":"Como executar","text":""},{"location":"iac/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<ul> <li>Azure CLI  </li> <li>Terraform  </li> <li>Conta Azure com permiss\u00f5es adequadas  </li> </ul>"},{"location":"iac/#passos-para-provisionar-cada-modulo","title":"Passos para provisionar cada m\u00f3dulo","text":"<p>Para provisionar os recursos, voc\u00ea deve entrar em cada pasta de m\u00f3dulo e executar os comandos do Terraform individualmente. Por exemplo, para o m\u00f3dulo <code>resource_group</code>:</p> <pre><code>cd resource_group  \nterraform init  \nterraform plan -var-file=\"../terraform.tfvars\"  \nterraform apply -var-file=\"../terraform.tfvars\"  \n</code></pre> <p>Repita esses passos para cada m\u00f3dulo (<code>sql_server</code>, <code>adls</code>, <code>az_databricks</code>, <code>adf</code>), sempre navegando para a respectiva pasta antes de executar os comandos.</p>"},{"location":"iac/#comandos-gerais-para-cada-modulo","title":"Comandos gerais para cada m\u00f3dulo","text":"<pre><code>terraform init  \nterraform plan -var-file=\"../terraform.tfvars\"  \nterraform apply -var-file=\"../terraform.tfvars\"  \n</code></pre>"},{"location":"iac/#exemplos-do-arquivo-terraformtfvars","title":"Exemplos do arquivo terraform.tfvars","text":"<p>Aqui est\u00e3o exemplos fict\u00edcios de valores para cada m\u00f3dulo no arquivo <code>terraform.tfvars</code>:</p>"},{"location":"iac/#resource_group","title":"resource_group","text":"<pre><code>resource_group_name = \"rg-exemplo-projeto\"  \nlocation            = \"eastus\"  \n</code></pre>"},{"location":"iac/#sql_server","title":"sql_server","text":"<pre><code>subscription_id     = \"12345678-1234-1234-1234-123456789abc\"  \nresource_group_name = \"rg-exemplo-projeto\"  \nusuario_admin       = \"admin_exemplo\"  \npassword            = \"SenhaForte!2025\"  \n</code></pre>"},{"location":"iac/#az_databricks","title":"az_databricks","text":"<pre><code>azure_client_id     = \"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee\"  \nazure_client_secret = \"AaBbCcDdEeFfGgHhIiJjKkLlMmNnOoPpQqRrSsTtUuVv\"  \nazure_tenant_id     = \"ffffffff-1111-2222-3333-444444444444\"  \nworkspace_name      = \"ws-exemplo-databricks\"  \nsubscription_id     = \"12345678-1234-1234-1234-123456789abc\"  \nresource_group_name = \"rg-exemplo-projeto\"  \n</code></pre>"},{"location":"iac/#adls","title":"adls","text":"<pre><code>subscription_id     = \"12345678-1234-1234-1234-123456789abc\"  \nresource_group_name = \"rg-exemplo-projeto\"  \n</code></pre>"},{"location":"iac/#adf","title":"adf","text":"<pre><code>resource_group_name = \"rg-exemplo-projeto\"  \ndata_factory_name   = \"adf-exemplo-projeto\"  \n</code></pre>"},{"location":"iac/#autenticacao-no-azure","title":"Autentica\u00e7\u00e3o no Azure","text":"<p>Antes de iniciar o provisionamento, \u00e9 necess\u00e1rio autenticar na sua conta Azure usando o Azure CLI:</p> <pre><code>az login  \n</code></pre> <p>Esse comando abrir\u00e1 uma janela no navegador para login. Ap\u00f3s logado, voc\u00ea poder\u00e1 executar comandos <code>az</code> para consultar os recursos e configurar vari\u00e1veis no <code>terraform.tfvars</code>.</p>"},{"location":"iac/#como-obter-os-valores-para-o-terraformtfvars","title":"Como obter os valores para o terraform.tfvars","text":"<p>Use os comandos abaixo para descobrir os valores usados nas vari\u00e1veis dos m\u00f3dulos:</p>"},{"location":"iac/#subscription_id","title":"subscription_id","text":"<pre><code>az account show --query id -o tsv  \n</code></pre>"},{"location":"iac/#resource_group_name","title":"resource_group_name","text":"<p><pre><code>az group list --query \"[].name\" -o tsv  \n</code></pre> recomendado utilizar o mesmo grupo de recursos para todos os servi\u00e7os provisionados.</p>"},{"location":"iac/#location","title":"location","text":"<pre><code>az account list-locations --query \"[].{Region:name}\" -o table  \n</code></pre>"},{"location":"iac/#azure_client_id-azure_client_secret-e-azure_tenant_id","title":"azure_client_id, azure_client_secret e azure_tenant_id","text":"<p>Para criar um novo Service Principal:</p> <pre><code>az ad sp create-for-rbac --name \"sp-nome-exemplo\" --role Contributor --scopes /subscriptions/&lt;subscription_id&gt;  \n</code></pre> <p>O retorno incluir\u00e1:</p> <ul> <li><code>appId</code> \u2192 <code>azure_client_id</code> </li> <li><code>password</code> \u2192 <code>azure_client_secret</code> </li> <li><code>tenant</code> \u2192 <code>azure_tenant_id</code> </li> </ul> <p>\u26a0\ufe0f Importante: guarde esses dados com seguran\u00e7a e nunca versiona esse arquivo no Git (o projeto ja possui um .gitigore com esse arquivo em cada pasta).</p>"},{"location":"iac/#consideracoes-finais","title":"Considera\u00e7\u00f5es finais","text":"<ul> <li>O arquivo <code>terraform.tfvars</code> na raiz deve conter as vari\u00e1veis de configura\u00e7\u00e3o utilizadas por todos os m\u00f3dulos.  </li> <li>Para destruir recursos, entre em cada m\u00f3dulo e rode:</li> </ul> <pre><code>terraform destroy -var-file=\"../terraform.tfvars\"  \n</code></pre>"},{"location":"notebooks/","title":"Documenta\u00e7\u00e3o dos Notebooks Databricks","text":"<p>Esta documenta\u00e7\u00e3o apresenta os notebooks que fazem o processo de ingest\u00e3o e tratamento dos dados em tr\u00eas camadas: Bronze, Silver e Gold .</p>"},{"location":"notebooks/#notebook-bronze","title":"Notebook Bronze","text":""},{"location":"notebooks/#objetivo","title":"Objetivo","text":"<p>Ingerir os arquivos CSV da landing zone no armazenamento Azure Data Lake (ADLS), montar os containers, carregar os dados em DataFrames Spark, aplicar algumas transforma\u00e7\u00f5es b\u00e1sicas e salvar no formato Delta na camada Bronze.</p>"},{"location":"notebooks/#configuracoes-e-montagem","title":"Configura\u00e7\u00f5es e Montagem","text":"<pre><code>storageAccountName = \"\"\nstorageAccountAccessKey = \"\"\nsasToken = \"\"\n\ndef mount_adls(blobContainerName):\n    try:\n        dbutils.fs.mount(\n            source = \"wasbs://{}@{}.blob.core.windows.net\".format(blobContainerName, storageAccountName),\n            mount_point = f\"/mnt/{storageAccountName}/{blobContainerName}\",\n            extra_configs = {'fs.azure.sas.' + blobContainerName + '.' + storageAccountName + '.blob.core.windows.net': sasToken}\n        )\n        print(\"OK!\")\n    except Exception as e:\n        print(\"Falha\", e)\n\nmount_adls('landing-zone')\nmount_adls('bronze')\nmount_adls('silver')\nmount_adls('gold')\n</code></pre>"},{"location":"notebooks/#ingestao-dos-dados-csv","title":"Ingest\u00e3o dos Dados CSV","text":"<p>Leitura dos arquivos CSV da landing zone para os DataFrames Spark e defini\u00e7\u00e3o dos nomes das colunas.</p> <p>Exemplo para o arquivo vendedores:</p> <pre><code>df_vendedores_raw = spark.read.option(\"header\", \"false\").csv(f\"/mnt/{storageAccountName}/landing-zone/ecommerce/vendedores.csv\")\ncolunas_vendedores = [\"id\", \"nome\", \"email\", \"telefone\", \"data_cadastro\"]\ndf_vendedores = df_vendedores_raw.toDF(*colunas_vendedores)\n</code></pre> <p>Processo similar para os demais arquivos:</p> <ul> <li>clientes.csv</li> <li>categorias.csv</li> <li>estoque.csv</li> <li>pagamentos.csv</li> <li>entregas.csv</li> <li>avaliacoes.csv</li> <li>pedidos.csv</li> <li>enderecos_cliente.csv</li> <li>transportadoras.csv</li> <li>formas_pagamento.csv</li> <li>itens_pedido.csv</li> <li>produtos.csv</li> </ul>"},{"location":"notebooks/#enriquecimento-dos-dataframes","title":"Enriquecimento dos DataFrames","text":"<p>Adi\u00e7\u00e3o das colunas <code>data_hora_bronze</code> (timestamp da ingest\u00e3o) e <code>nome_arquivo</code> para rastreamento.</p> <pre><code>from pyspark.sql.functions import current_timestamp, lit\n\ndf_vendedores = df_vendedores.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"vendedores.csv\"))\n# Mesmo para os demais DataFrames...\n</code></pre>"},{"location":"notebooks/#sanitizacao-dos-nomes-das-colunas","title":"Sanitiza\u00e7\u00e3o dos nomes das colunas","text":"<p>Fun\u00e7\u00e3o para padronizar os nomes das colunas:</p> <pre><code>def sanitize_columns(df):\n    for col_name in df.columns:\n        new_name = (\n            col_name.strip()\n            .lower()\n            .replace(\" \", \"_\")\n            .replace(\"(\", \"\")\n            .replace(\")\", \"\")\n            .replace(\"-\", \"_\")\n            .replace(\",\", \"\")\n            .replace(\";\", \"\")\n            .replace(\"{\", \"\")\n            .replace(\"}\", \"\")\n            .replace(\"=\", \"\")\n            .replace(\"\\n\", \"\")\n            .replace(\"\\t\", \"\")\n        )\n        df = df.withColumnRenamed(col_name, new_name)\n    return df\n</code></pre>"},{"location":"notebooks/#salvamento-dos-dados-na-camada-bronze","title":"Salvamento dos dados na camada Bronze","text":"<pre><code>dfs = [\n    (df_vendedores, \"vendedores\"),\n    (df_clientes, \"clientes\"),\n    (df_categorias, \"categorias\"),\n    (df_estoque, \"estoque\"),\n    (df_pagamentos, \"pagamentos\"),\n    (df_entregas, \"entregas\"),\n    (df_avaliacoes, \"avaliacoes\"),\n    (df_formas_pagamento, \"formas_pagamento\"),\n    (df_transportadoras, \"transportadoras\"),\n    (df_pedidos, \"pedidos\"),\n    (df_enderecos_cliente, \"enderecos_cliente\"),\n    (df_itens_pedido, \"itens_pedido\"),\n    (df_produtos, \"produtos\"),\n]\n\nfor df, name in dfs:\n    df_sanitized = sanitize_columns(df)\n    path = f\"/mnt/{storageAccountName}/bronze/ecommerce/{name}\"\n    df_sanitized.write.format(\"delta\").save(path)\n</code></pre>"},{"location":"notebooks/#notebook-silver","title":"Notebook Silver","text":""},{"location":"notebooks/#objetivo_1","title":"Objetivo","text":"<p>Ler os dados da camada Bronze, aplicar tratamentos e renomea\u00e7\u00f5es, adicionar metadados, e salvar os dados processados na camada Silver.</p>"},{"location":"notebooks/#montagem-dos-containers-mesma-funcao-do-notebook-bronze","title":"Montagem dos containers (mesma fun\u00e7\u00e3o do notebook Bronze)","text":"<pre><code>storageAccountName = \"\"\nstorageAccountAccessKey = \"\"\nsasToken= \"\"\n\ndef mount_adls(blobContainerName):\n    try:\n        dbutils.fs.mount(\n            source = \"wasbs://{}@{}.blob.core.windows.net\".format(blobContainerName, storageAccountName),\n            mount_point = f\"/mnt/{storageAccountName}/{blobContainerName}\",\n            extra_configs = {'fs.azure.sas.' + blobContainerName + '.' + storageAccountName + '.blob.core.windows.net': sasToken}\n        )\n        print(\"OK!\")\n    except Exception as e:\n        print(\"Falha\", e)\n</code></pre>"},{"location":"notebooks/#leitura-dos-dados-bronze-em-formato-delta","title":"Leitura dos dados Bronze em formato Delta","text":"<pre><code>df_avaliacoes          = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/avaliacoes\")\ndf_categorias          = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/categorias\")\ndf_clientes            = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/clientes\")\ndf_enderecos_cliente   = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/enderecos_cliente\")\ndf_entregas            = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/entregas\")\ndf_estoque             = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/estoque\")\ndf_formas_pagamento    = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/formas_pagamento\")\ndf_itens_pedidos       = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/itens_pedido\")\ndf_pagamentos          = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/pagamentos\")\ndf_pedidos             = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/pedidos\")\ndf_produtos            = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/produtos\")\ndf_transportadoras     = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/transportadoras\")\ndf_vendedores          = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/vendedores\")\n</code></pre>"},{"location":"notebooks/#adicao-de-colunas-de-metadados-para-silver","title":"Adi\u00e7\u00e3o de colunas de metadados para Silver","text":"<pre><code>from pyspark.sql.functions import current_timestamp, lit\n\ndf_avaliacoes = df_avaliacoes.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"avaliacoes\"))\n# Repetir para os demais DataFrames\n</code></pre>"},{"location":"notebooks/#renomeacao-das-colunas-para-padrao-maiusculo-e-tratamento-de-sufixos","title":"Renomea\u00e7\u00e3o das colunas para padr\u00e3o mai\u00fasculo e tratamento de sufixos","text":"<p>Fun\u00e7\u00e3o para renomear as colunas:</p> <pre><code>from pyspark.sql.functions import lit, current_timestamp\n\ndef renomear_colunas(diretorio):\n    df = spark.read.format('delta').load(diretorio)\n    tabela = diretorio.split('/')[-2]\n\n    novos_nomes = {}\n\n    for coluna in df.columns:\n        novo_nome = coluna.upper()\n\n        if novo_nome.endswith(\"_ID\"):\n            prefixo = novo_nome[:-3]\n            novo_nome = f\"CODIGO_{prefixo}\"\n        else:\n            novo_nome = novo_nome.replace(\"ID\", \"CODIGO\")\n\n        novos_nomes[coluna] = novo_nome\n\n    for antigo, novo in novos_nomes.items():\n        df = df.withColumnRenamed(antigo, novo)\n\n    for col_drop in [\"DATA_HORA_BRONZE\", \"NOME_ARQUIVO\"]:\n        if col_drop in df.columns:\n            df = df.drop(col_drop)\n\n    df = df.withColumn(\"NOME_ARQUIVO_BRONZE\", lit(tabela))\n    df = df.withColumn(\"DATA_ARQUIVO_SILVER\", current_timestamp())\n\n    df.write.format('delta').mode(\"overwrite\").save(f\"/mnt/{storageAccountName}/silver/ecommerce/{tabela}\")\n\ndef renomear_arquivos_delta(diretorio):\n    arquivos = dbutils.fs.ls(diretorio)\n    for arquivo in arquivos:\n        renomear_colunas(arquivo.path)\n\ndiretorio = f'/mnt/{storageAccountName}/bronze/ecommerce'\nrenomear_arquivos_delta(diretorio)\n</code></pre>"},{"location":"notebooks/#notebook-gold","title":"Notebook Gold","text":""},{"location":"notebooks/#objetivo_2","title":"Objetivo","text":"<p>Ler os dados da camada silver, fazer algumas agrega\u00e7\u00f5es entre tabelas e disponibilizar um modelo dimensional, com tabelas de dimens\u00e3o e tabelas fato, ja com todos os dados em formatos delta prontos para serem consumidos por Power BI ou outra ferramenta de visualiza\u00e7\u00e3o de dados.</p>"},{"location":"notebooks/#montagem-dos-containers-mesma-funcao-do-notebook-silver","title":"Montagem dos containers (mesma fun\u00e7\u00e3o do notebook Silver)","text":"<pre><code>storageAccountName = \"\"\nstorageAccountAccessKey = \"\"\nsasToken= \"\"\n\ndef mount_adls(blobContainerName):\n    try:\n        dbutils.fs.mount(\n            source = \"wasbs://{}@{}.blob.core.windows.net\".format(blobContainerName, storageAccountName),\n            mount_point = f\"/mnt/{storageAccountName}/{blobContainerName}\",\n            extra_configs = {'fs.azure.sas.' + blobContainerName + '.' + storageAccountName + '.blob.core.windows.net': sasToken}\n        )\n        print(\"OK!\")\n    except Exception as e:\n        print(\"Falha\", e)\n</code></pre>"},{"location":"notebooks/#leitura-em-dos-dados-da-silver-em-formato-delta","title":"Leitura em dos dados da Silver em formato Delta","text":"<pre><code>df_avaliacoes          = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/ecommerce/avaliacoes\")\ndf_categorias          = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/ecommerce/categorias\")\ndf_clientes            = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/ecommerce/clientes\")\ndf_enderecos_cliente  = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/ecommerce/enderecos_cliente\")\ndf_entregas            = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/ecommerce/entregas\")\ndf_estoque             = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/ecommerce/estoque\")\ndf_formas_pagamento    = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/ecommerce/formas_pagamento\")\ndf_itens_pedidos       = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/ecommerce/itens_pedido\")\ndf_pagamentos          = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/ecommerce/pagamentos\")\ndf_pedidos             = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/ecommerce/pedidos\")\ndf_produtos            = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/ecommerce/produtos\")\ndf_transportadoras     = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/ecommerce/transportadoras\")\ndf_vendedores          = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/ecommerce/vendedores\")\n</code></pre>"},{"location":"notebooks/#adiciona-metadados-para-os-dataframes-utiizados-no-notebook","title":"Adiciona metadados para os dataframes utiizados no notebook","text":"<p>from pyspark.sql.functions import current_timestamp, lit</p> <pre><code>df_avaliacoes          = df_avaliacoes.withColumn(\"data_hora_gold\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"avaliacoes\"))\ndf_categorias          = df_categorias.withColumn(\"data_hora_gold\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"categorias\"))\ndf_clientes            = df_clientes.withColumn(\"data_hora_gold\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"clientes\"))\ndf_entregas            = df_entregas.withColumn(\"data_hora_gold\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"entregas\"))\ndf_estoque             = df_estoque.withColumn(\"data_hora_gold\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"estoque\"))\ndf_formas_pagamento    = df_formas_pagamento.withColumn(\"data_hora_gold\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"formas_pagamento\"))\ndf_itens_pedidos       = df_itens_pedidos.withColumn(\"data_hora_gold\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"itens_pedidos\"))\ndf_pagamentos          = df_pagamentos.withColumn(\"data_hora_gold\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"pagamentos\"))\ndf_pedidos             = df_pedidos.withColumn(\"data_hora_gold\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"pedidos\"))\ndf_produtos            = df_produtos.withColumn(\"data_hora_gold\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"produtos\"))\ndf_transportadoras     = df_transportadoras.withColumn(\"data_hora_gold\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"transportadoras\"))\ndf_vendedores          = df_vendedores.withColumn(\"data_hora_gold\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"vendedores\"))\n</code></pre>"},{"location":"notebooks/#cria-as-tabelas-dimensionais","title":"Cria as tabelas dimensionais","text":""},{"location":"notebooks/#tabela-dimensao-clientes","title":"Tabela Dimens\u00e3o clientes","text":"<pre><code>%sql\nCREATE TABLE IF NOT EXISTS dim_clientes (\n  ID BIGINT GENERATED ALWAYS AS IDENTITY,\n  CODIGO_CLIENTE INT,\n  NOME STRING,\n  EMAIL STRING,\n  TELEFONE STRING,\n  CIDADE STRING,\n  ESTADO STRING,\n  DATA_CADASTRO TIMESTAMP\n)\nUSING delta\nLOCATION '/mnt/datalake922b9abd80170b5b/gold/ecommerce/dim_clientes';\n</code></pre>"},{"location":"notebooks/#tabela-dimensao-produtos","title":"Tabela Dimens\u00e3o produtos","text":"<pre><code>%sql\nCREATE TABLE IF NOT EXISTS dim_produtos (\n  ID BIGINT GENERATED ALWAYS AS IDENTITY,  -- Chave substituta (SK)\n  CODIGO_PRODUTO INT,                      -- Chave natural do sistema transacional\n  NOME STRING,\n  DESCRICAO STRING,\n  PRECO DECIMAL(10,2),\n  CATEGORIA STRING,\n  DATA_CADASTRO TIMESTAMP\n)\nUSING delta\nLOCATION '/mnt/datalake922b9abd80170b5b/gold/ecommerce/dim_produtos';\n</code></pre>"},{"location":"notebooks/#tabela-dimensao-vendedores","title":"Tabela Dimens\u00e3o vendedores","text":"<pre><code>%sql\nCREATE TABLE IF NOT EXISTS dim_vendedores (\n  ID BIGINT GENERATED ALWAYS AS IDENTITY,   -- Chave substituta (SK)\n  CODIGO_VENDEDOR INT,                      -- Chave natural do sistema de origem\n  NOME STRING,\n  EMAIL STRING,\n  TELEFONE STRING,\n  DATA_CADASTRO TIMESTAMP\n)\nUSING delta\nLOCATION '/mnt/datalake922b9abd80170b5b/gold/ecommerce/dim_vendedores';\n</code></pre>"},{"location":"notebooks/#tabela-dimensao-formas-de-pagamento","title":"Tabela Dimens\u00e3o formas de pagamento","text":"<pre><code>%sql\nCREATE TABLE IF NOT EXISTS dim_formas_pagamento (\n  ID BIGINT GENERATED ALWAYS AS IDENTITY,  -- Chave substituta (SK)\n  CODIGO_FORMA INT,                        -- Chave natural (caso exista)\n  DESCRICAO STRING\n)\nUSING delta\nLOCATION '/mnt/datalake922b9abd80170b5b/gold/ecommerce/dim_formas_pagamento';\n</code></pre>"},{"location":"notebooks/#tabela-dimensao-tempo","title":"Tabela Dimens\u00e3o tempo","text":"<pre><code>from pyspark.sql.functions import expr, date_format, year, month, dayofmonth, dayofweek, when, col\n\ndata_inicial = \"2022-01-01\"\ndata_final = \"2025-12-31\"\n\nnum_dias = spark.sql(f\"SELECT datediff('{data_final}', '{data_inicial}')\").collect()[0][0]\n\ndf_calendario = spark.range(0, num_dias + 1) \\\n    .selectExpr(f\"date_add(to_date('{data_inicial}'), CAST(id AS INT)) AS Data\")\n\n# Criar colunas nome m\u00eas e dia da semana com when (case when)\ndf_tempo = df_calendario.select(\n    col(\"Data\"),\n    year(\"Data\").alias(\"Ano\"),\n    month(\"Data\").alias(\"Mes\"),\n    when(month(\"Data\") == 1, \"JANEIRO\")\n    .when(month(\"Data\") == 2, \"FEVEREIRO\")\n    .when(month(\"Data\") == 3, \"MARCO\")\n    .when(month(\"Data\") == 4, \"ABRIL\")\n    .when(month(\"Data\") == 5, \"MAIO\")\n    .when(month(\"Data\") == 6, \"JUNHO\")\n    .when(month(\"Data\") == 7, \"JULHO\")\n    .when(month(\"Data\") == 8, \"AGOSTO\")\n    .when(month(\"Data\") == 9, \"SETEMBRO\")\n    .when(month(\"Data\") == 10, \"OUTUBRO\")\n    .when(month(\"Data\") == 11, \"NOVEMBRO\")\n    .when(month(\"Data\") == 12, \"DEZEMBRO\")\n    .alias(\"NomeMes\"),\n    dayofmonth(\"Data\").alias(\"Dia\"),\n    when(dayofweek(\"Data\") == 1, \"DOMINGO\")\n    .when(dayofweek(\"Data\") == 2, \"SEGUNDA-FEIRA\")\n    .when(dayofweek(\"Data\") == 3, \"TERCA-FEIRA\")\n    .when(dayofweek(\"Data\") == 4, \"QUARTA-FEIRA\")\n    .when(dayofweek(\"Data\") == 5, \"QUINTA-FEIRA\")\n    .when(dayofweek(\"Data\") == 6, \"SEXTA-FEIRA\")\n    .when(dayofweek(\"Data\") == 7, \"SABADO\")\n    .alias(\"NomeDiaSemana\"),\n    dayofweek(\"Data\").alias(\"NumeroDiaSemana\"),\n    date_format(\"Data\", \"yyyyMMdd\").cast(\"int\").alias(\"ID\")\n)\n\ndf_tempo.display()\n\ndf_tempo.write.mode(\"overwrite\")\\\n    .option(\"path\", f\"/mnt/{storageAccountName}/gold/ecommerce/dim_tempo\")\\\n    .saveAsTable(\"dim_tempo\")\n</code></pre>"},{"location":"notebooks/#tabela-dimensao-entregas","title":"Tabela Dimens\u00e3o entregas","text":"<pre><code>%sql\nCREATE TABLE IF NOT EXISTS dim_entregas (\n  ID BIGINT GENERATED ALWAYS AS IDENTITY,  -- Chave substituta (SK)\n  CODIGO_ENTREGA INT,                       -- Chave natural (caso exista)\n  TRANSPORTADORA STRING,\n  STATUS STRING,\n  DATA_ENVIO TIMESTAMP,\n  DATA_ENTREGA TIMESTAMP\n)\nUSING delta\nLOCATION '/mnt/datalake922b9abd80170b5b/gold/ecommerce/dim_entregas';\n</code></pre>"},{"location":"notebooks/#cria-os-merges-e-faz-os-inserts-nas-tabelas-a-partir-de-tabelas-temporarias-criadas-com-os-dataframes","title":"Cria os merges e faz os inserts nas tabelas a partir de tabelas tempor\u00e1rias criadas com os dataframes","text":"<pre><code>df_clientes.createOrReplaceTempView(\"silver_clientes\")\ndf_enderecos_cliente.createOrReplaceTempView(\"silver_enderecos_cliente\")\ndf_produtos.createOrReplaceTempView(\"silver_produtos\")\ndf_categorias.createOrReplaceTempView(\"silver_categorias\")\ndf_vendedores.createOrReplaceTempView(\"silver_vendedores\")\ndf_formas_pagamento.createOrReplaceTempView(\"silver_formas_pagamento\")\ndf_entregas.createOrReplaceTempView(\"silver_entregas\")\ndf_transportadoras.createOrReplaceTempView(\"silver_transportadoras\")\ndf_pedidos.createOrReplaceTempView(\"pedidos\")\ndf_itens_pedidos.createOrReplaceTempView(\"itens_pedidos\")\ndf_pagamentos.createOrReplaceTempView(\"pagamentos\")\ndf_avaliacoes.createOrReplaceTempView(\"avaliacoes\")\ndf_produtos.createOrReplaceTempView(\"produtos\")\n</code></pre>"},{"location":"notebooks/#tabela-dimensao-clientes_1","title":"Tabela Dimens\u00e3o clientes","text":"<pre><code>%sql\n-- Criar TEMP VIEW com os dados de origem\nCREATE OR REPLACE TEMP VIEW dim_clientes_temp AS\nSELECT\n    c.CODIGO AS CODIGO_CLIENTE,\n    c.NOME,\n    c.EMAIL,\n    c.TELEFONE,\n    ec.CIDADE,\n    ec.ESTADO,\n    c.DATA_CADASTRO\nFROM silver_clientes c\nLEFT JOIN silver_enderecos_cliente ec ON c.CODIGO = ec.CODIGO_CLIENTE;\n\n-- MERGE na tabela de dimens\u00e3o usando a chave natural\nMERGE INTO dim_clientes AS target\nUSING dim_clientes_temp AS source\nON target.CODIGO_CLIENTE = source.CODIGO_CLIENTE\nWHEN MATCHED THEN\n  UPDATE SET\n    target.NOME = source.NOME,\n    target.EMAIL = source.EMAIL,\n    target.TELEFONE = source.TELEFONE,\n    target.CIDADE = source.CIDADE,\n    target.ESTADO = source.ESTADO,\n    target.DATA_CADASTRO = source.DATA_CADASTRO\nWHEN NOT MATCHED THEN\n  INSERT (CODIGO_CLIENTE, NOME, EMAIL, TELEFONE, CIDADE, ESTADO, DATA_CADASTRO)\n  VALUES (source.CODIGO_CLIENTE, source.NOME, source.EMAIL, source.TELEFONE, source.CIDADE, source.ESTADO, source.DATA_CADASTRO);\n</code></pre>"},{"location":"notebooks/#tabela-dimensao-produtos_1","title":"Tabela Dimens\u00e3o produtos","text":"<pre><code>%sql\n-- Criar ou substituir a TEMP VIEW com os dados transformados\nCREATE OR REPLACE TEMP VIEW dim_produtos_temp AS\nSELECT\n    p.CODIGO AS CODIGO_PRODUTO,\n    p.NOME,\n    p.DESCRICAO,\n    p.PRECO,\n    c.NOME AS CATEGORIA,\n    p.DATA_CADASTRO\nFROM silver_produtos p\nLEFT JOIN silver_categorias c ON p.CODIGO_CATEGORIA = c.CODIGO;\n\n-- Realizar o MERGE na tabela de dimens\u00e3o\nMERGE INTO dim_produtos AS target\nUSING dim_produtos_temp AS source\nON target.CODIGO_PRODUTO = source.CODIGO_PRODUTO\nWHEN MATCHED THEN\n  UPDATE SET\n    target.NOME = source.NOME,\n    target.DESCRICAO = source.DESCRICAO,\n    target.PRECO = source.PRECO,\n    target.CATEGORIA = source.CATEGORIA,\n    target.DATA_CADASTRO = source.DATA_CADASTRO\nWHEN NOT MATCHED THEN\n  INSERT (CODIGO_PRODUTO, NOME, DESCRICAO, PRECO, CATEGORIA, DATA_CADASTRO)\n  VALUES (source.CODIGO_PRODUTO, source.NOME, source.DESCRICAO, source.PRECO, source.CATEGORIA, source.DATA_CADASTRO);\n</code></pre>"},{"location":"notebooks/#tabela-dimensao-vendedores_1","title":"Tabela Dimens\u00e3o vendedores","text":"<pre><code>%sql\n-- Criar ou substituir a TEMP VIEW com os dados transformados\nCREATE OR REPLACE TEMP VIEW dim_vendedores_temp AS\nSELECT\n    CODIGO AS CODIGO_VENDEDOR,\n    NOME,\n    EMAIL,\n    TELEFONE,\n    DATA_CADASTRO\nFROM silver_vendedores;\n\n-- Realizar o MERGE na tabela de dimens\u00e3o\nMERGE INTO dim_vendedores AS target\nUSING dim_vendedores_temp AS source\nON target.CODIGO_VENDEDOR = source.CODIGO_VENDEDOR\nWHEN MATCHED THEN\n  UPDATE SET\n    target.NOME = source.NOME,\n    target.EMAIL = source.EMAIL,\n    target.TELEFONE = source.TELEFONE,\n    target.DATA_CADASTRO = source.DATA_CADASTRO\nWHEN NOT MATCHED THEN\n  INSERT (CODIGO_VENDEDOR, NOME, EMAIL, TELEFONE, DATA_CADASTRO)\n  VALUES (source.CODIGO_VENDEDOR, source.NOME, source.EMAIL, source.TELEFONE, source.DATA_CADASTRO);\n</code></pre>"},{"location":"notebooks/#tabela-dimensao-formas-de-pagamento_1","title":"Tabela Dimens\u00e3o formas de pagamento","text":"<pre><code>%sql\n-- Criar ou substituir a TEMP VIEW com dados da origem\nCREATE OR REPLACE TEMP VIEW dim_formas_pagamento_temp AS\nSELECT\n    CODIGO AS CODIGO_FORMA,\n    DESCRICAO\nFROM silver_formas_pagamento;\n\n-- MERGE na dimens\u00e3o dim_formas_pagamento\nMERGE INTO dim_formas_pagamento AS target\nUSING dim_formas_pagamento_temp AS source\nON target.CODIGO_FORMA = source.CODIGO_FORMA\nWHEN MATCHED THEN\n  UPDATE SET\n    target.DESCRICAO = source.DESCRICAO\nWHEN NOT MATCHED THEN\n  INSERT (CODIGO_FORMA, DESCRICAO)\n  VALUES (source.CODIGO_FORMA, source.DESCRICAO);\n</code></pre>"},{"location":"notebooks/#tabela-dimensao-entregas_1","title":"Tabela Dimens\u00e3o entregas","text":"<pre><code>%sql\n-- Criar ou substituir a TEMP VIEW com os dados de origem\nCREATE OR REPLACE TEMP VIEW dim_entregas_temp AS\nSELECT\n    e.CODIGO AS CODIGO_ENTREGA,\n    t.NOME AS TRANSPORTADORA,\n    e.STATUS,\n    e.DATA_ENVIO,\n    e.DATA_ENTREGA\nFROM silver_entregas e\nLEFT JOIN silver_transportadoras t ON e.CODIGO_TRANSPORTADORA = t.CODIGO;\n\n-- Executar o MERGE na tabela de dimens\u00e3o\nMERGE INTO dim_entregas AS target\nUSING dim_entregas_temp AS source\nON target.CODIGO_ENTREGA = source.CODIGO_ENTREGA\nWHEN MATCHED THEN\n  UPDATE SET\n    target.TRANSPORTADORA = source.TRANSPORTADORA,\n    target.STATUS = source.STATUS,\n    target.DATA_ENVIO = source.DATA_ENVIO,\n    target.DATA_ENTREGA = source.DATA_ENTREGA\nWHEN NOT MATCHED THEN\n  INSERT (CODIGO_ENTREGA, TRANSPORTADORA, STATUS, DATA_ENVIO, DATA_ENTREGA)\n  VALUES (source.CODIGO_ENTREGA, source.TRANSPORTADORA, source.STATUS, source.DATA_ENVIO, source.DATA_ENTREGA);\n</code></pre>"},{"location":"notebooks/#criacao-da-tabela-fato-vendas","title":"Cria\u00e7\u00e3o da tabela Fato vendas","text":"<pre><code>%sql\nCREATE TABLE IF NOT EXISTS fato_vendas (\n  ID BIGINT GENERATED ALWAYS AS IDENTITY,  -- SK autom\u00e1tica\n  CLIENTE_SK INT,\n  PRODUTO_SK INT,\n  VENDEDOR_SK INT,\n  TEMPO_SK INT,\n  FORMA_PAGAMENTO_SK INT,\n  ENTREGA_SK INT,\n  QUANTIDADE INT,\n  PRECO_UNITARIO DECIMAL(10,2),\n  VALOR_TOTAL DECIMAL(12,2),\n  TEMPO_ENTREGA_DIAS INT,\n  NOTA_AVALIACAO INT\n)\nUSING delta\nLOCATION '/mnt/datalake922b9abd80170b5b/gold/ecommerce/fato_vendas';\n</code></pre> <p>Depois cria\u00e7\u00e3o de views tempor\u00e1ria</p> <pre><code>spark.read.format(\"delta\").load(\"/mnt/datalake922b9abd80170b5b/gold/ecommerce/dim_clientes\").createOrReplaceTempView(\"dim_clientes\")\nspark.read.format(\"delta\").load(\"/mnt/datalake922b9abd80170b5b/gold/ecommerce/dim_produtos\").createOrReplaceTempView(\"dim_produtos\")\nspark.read.format(\"delta\").load(\"/mnt/datalake922b9abd80170b5b/gold/ecommerce/dim_vendedores\").createOrReplaceTempView(\"dim_vendedores\")\nspark.read.format(\"delta\").load(\"/mnt/datalake922b9abd80170b5b/gold/ecommerce/dim_tempo\").createOrReplaceTempView(\"dim_tempo\")\nspark.read.format(\"delta\").load(\"/mnt/datalake922b9abd80170b5b/gold/ecommerce/dim_formas_pagamento\").createOrReplaceTempView(\"dim_formas_pagamento\")\nspark.read.format(\"delta\").load(\"/mnt/datalake922b9abd80170b5b/gold/ecommerce/dim_entregas\").createOrReplaceTempView(\"dim_entregas\")\n</code></pre> <p>e por fim adiciona os dados na tabela fato</p> <pre><code>%sql\nMERGE INTO fato_vendas AS target\nUSING (\n  SELECT\n    dc.ID AS CLIENTE_SK,\n    dp.ID AS PRODUTO_SK,\n    dv.ID AS VENDEDOR_SK,\n    dt.ID AS TEMPO_SK,\n    dfp.ID AS FORMA_PAGAMENTO_SK,\n    de.ID AS ENTREGA_SK,\n    ip.QUANTIDADE,\n    ip.PRECO_UNITARIO,\n    ip.QUANTIDADE * ip.PRECO_UNITARIO AS VALOR_TOTAL,\n    DATEDIFF(de.DATA_ENTREGA, de.DATA_ENVIO) AS TEMPO_ENTREGA_DIAS,\n    av.NOTA AS NOTA_AVALIACAO\n  FROM pedidos p\n  INNER JOIN itens_pedidos ip ON ip.CODIGO_PEDIDO = p.CODIGO\n  INNER JOIN dim_clientes dc ON dc.CODIGO_CLIENTE = p.CODIGO_CLIENTE\n  INNER JOIN dim_produtos dp ON dp.CODIGO_PRODUTO = ip.CODIGO_PRODUTO\n  INNER JOIN produtos pr ON pr.CODIGO = ip.CODIGO_PRODUTO\n  INNER JOIN dim_vendedores dv ON dv.CODIGO_VENDEDOR = pr.CODIGO_VENDEDOR\n  INNER JOIN dim_tempo dt ON dt.Data = CAST(p.DATA_PEDCODIGOO AS DATE)\n  LEFT JOIN pagamentos pg ON pg.CODIGO_PEDIDO = p.CODIGO\n  LEFT JOIN dim_formas_pagamento dfp ON dfp.CODIGO_FORMA = pg.CODIGO\n  LEFT JOIN dim_entregas de ON de.CODIGO_ENTREGA = p.CODIGO_ENDERECO_ENTREGA\n  LEFT JOIN (\n    SELECT *,\n           ROW_NUMBER() OVER (PARTITION BY CODIGO_PRODUTO, CODIGO_CLIENTE ORDER BY DATA_AVALIACAO DESC) AS rn\n    FROM avaliacoes\n  ) av ON av.CODIGO_PRODUTO = ip.CODIGO_PRODUTO\n       AND av.CODIGO_CLIENTE = p.CODIGO_CLIENTE\n       AND av.rn = 1\n) AS source\nON \n  target.CLIENTE_SK = source.CLIENTE_SK\n  AND target.PRODUTO_SK = source.PRODUTO_SK\n  AND target.VENDEDOR_SK = source.VENDEDOR_SK\n  AND target.TEMPO_SK = source.TEMPO_SK\n  AND target.FORMA_PAGAMENTO_SK = source.FORMA_PAGAMENTO_SK\n  AND target.ENTREGA_SK = source.ENTREGA_SK\n  AND target.QUANTIDADE = source.QUANTIDADE\n  AND target.PRECO_UNITARIO = source.PRECO_UNITARIO\nWHEN NOT MATCHED THEN\nINSERT (\n  CLIENTE_SK,\n  PRODUTO_SK,\n  VENDEDOR_SK,\n  TEMPO_SK,\n  FORMA_PAGAMENTO_SK,\n  ENTREGA_SK,\n  QUANTIDADE,\n  PRECO_UNITARIO,\n  VALOR_TOTAL,\n  TEMPO_ENTREGA_DIAS,\n  NOTA_AVALIACAO\n)\nVALUES (\n  source.CLIENTE_SK,\n  source.PRODUTO_SK,\n  source.VENDEDOR_SK,\n  source.TEMPO_SK,\n  source.FORMA_PAGAMENTO_SK,\n  source.ENTREGA_SK,\n  source.QUANTIDADE,\n  source.PRECO_UNITARIO,\n  source.VALOR_TOTAL,\n  source.TEMPO_ENTREGA_DIAS,\n  source.NOTA_AVALIACAO\n);\n</code></pre>"},{"location":"powerbi/","title":"Documenta\u00e7\u00e3o Power BI - Projeto Ecommerce","text":"<p>Este documento descreve os indicadores, m\u00e9tricas e fontes de dados utilizadas no dashboard Power BI constru\u00eddo sobre a camada Gold do Data Lake.</p>"},{"location":"powerbi/#objetivo","title":"Objetivo","text":"<p>Consumir os dados processados da camada Gold no Power BI para an\u00e1lise de vendas, desempenho por produto, comportamento do cliente e efici\u00eancia da opera\u00e7\u00e3o log\u00edstica e financeira.</p>"},{"location":"powerbi/#conexao-com-os-dados","title":"Conex\u00e3o com os Dados","text":"<p>Os dados est\u00e3o armazenados em formato Delta no Azure Data Lake Storage (ADLS) Gen2, organizados nas seguintes tabelas dimensionais e fato:</p> Tabela Caminho no Data Lake Fato Vendas fato_vendas Clientes dim_clientes Entregas dim_entregas Formas de Pagamento dim_formas_pagamento Produtos dim_produtos Tempo dim_tempo Vendedores dim_vendedores"},{"location":"powerbi/#kpis-utilizados","title":"KPIs Utilizados","text":"<p>As principais m\u00e9tricas de desempenho utilizadas no relat\u00f3rio incluem:</p> <ul> <li> <p>Clientes \u00danicos <code>Clientes \u00danicos = DISTINCTCOUNT(fato_vendas[cliente_sk])</code></p> </li> <li> <p>Quantidade Total de Itens Vendidos <code>Quantidade Total = SUM(fato_vendas[quantidade])</code></p> </li> <li> <p>Receita Total <code>Receita Total = SUM(fato_vendas[valor_total])</code></p> </li> <li> <p>Ticket M\u00e9dio <code>Ticket M\u00e9dio = DIVIDE([Receita Total], COUNT(fato_vendas[id]))</code></p> </li> </ul>"},{"location":"powerbi/#metricas-por-dimensao","title":"M\u00e9tricas por Dimens\u00e3o","text":"<p>Essas m\u00e9tricas possibilitam cortes estrat\u00e9gicos por diferentes dimens\u00f5es de neg\u00f3cio:</p> <ul> <li> <p>Receita Total por Categoria de Produto   Dimens\u00e3o: <code>dim_produtos[categoria]</code>   Medida: <code>Receita Total</code></p> </li> <li> <p>Receita Total por Ano   Dimens\u00e3o: <code>dim_tempo[data]</code>   Medida: <code>Receita Total</code></p> </li> </ul>"},{"location":"powerbi/#consideracoes-finais","title":"Considera\u00e7\u00f5es Finais","text":"<p>A camada Gold fornece dados tratados, limpos e modelados prontos para consumo anal\u00edtico. A conex\u00e3o no Power BI foi feita utilizando o conector Azure Data Lake Gen2 com autentica\u00e7\u00e3o baseada em chave ou OAuth, dependendo do ambiente.</p> <p>Para visualizar o dashboard, clique aqui.</p>"}]}