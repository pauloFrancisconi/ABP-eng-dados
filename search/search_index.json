{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Documenta\u00e7\u00e3o do Projeto de Engenharia de Dados","text":"<p>Este projeto consiste em uma arquitetura de engenharia de dados na nuvem utilizando os principais servi\u00e7os da Azure. O fluxo contempla ingest\u00e3o, armazenamento, transforma\u00e7\u00e3o e visualiza\u00e7\u00e3o de dados.</p>"},{"location":"#ferramentas-utilizadas","title":"Ferramentas utilizadas","text":"<ul> <li>Banco de dados: Azure SQL Database</li> <li>Orquestra\u00e7\u00e3o: Azure Data Factory</li> <li>Processamento: Azure Databricks</li> <li>Armazenamento: Azure Data Lake Storage Gen2</li> <li>Visualiza\u00e7\u00e3o: Power BI</li> </ul>"},{"location":"#estrutura-do-repositorio","title":"Estrutura do Reposit\u00f3rio","text":"<ul> <li><code>data/</code>: Scripts SQL para cria\u00e7\u00e3o do banco de dados relacional, schema e tabelas. Diagramas ER tamb\u00e9m podem ser adicionados a partir de <code>/assets</code>.</li> <li><code>Iac/</code>: Infraestrutura como C\u00f3digo (IaC) com Terraform para provisionamento dos recursos na Azure.</li> <li>Subpastas para cada servi\u00e7o: <code>adls/</code>, <code>sql_server/</code>, <code>az_databricks/</code>, <code>adf/</code>, <code>resource_group/</code>.</li> <li><code>assets/</code>: Imagens, diagramas ER e arquivos auxiliares.</li> <li><code>docs/</code>: Documenta\u00e7\u00e3o completa da aplica\u00e7\u00e3o (este diret\u00f3rio).</li> <li><code>src/</code>: Pasta com subpastas <code>faker/</code>: Script Python que gera dados fict\u00edcios e insere no banco SQL e <code>notebooks/</code>: Notebooks Python com os scripts de transforma\u00e7\u00e3o de arquivos no ADLS Gen2 utilizando Azure Databricks.</li> </ul>"},{"location":"#conteudo-da-documentacao","title":"Conte\u00fado da Documenta\u00e7\u00e3o","text":"<ul> <li>Vis\u00e3o Geral da Arquitetura</li> <li>Provisionamento com Terraform (IaC)</li> <li>Banco de Dados e Diagramas ER</li> <li>Cria\u00e7\u00e3o de Dados com Faker</li> <li>Notebooks do Databricks (ETL)</li> <li>Pipelines no Azure Data Factory - Configura\u00e7\u00e3o</li> <li>Pipelines no Azure Data Factory - Pipeline</li> <li>Dashboard no Power BI</li> </ul>"},{"location":"adf-config/","title":"Azure Data Factory - Integra\u00e7\u00e3o de Dados","text":"<p>Este documento explica como acessar o Azure Data Factory criado via Terraform, configurar os servi\u00e7os vinculados (Linked Services) e criar os conjuntos de dados (Datasets) necess\u00e1rios para a movimenta\u00e7\u00e3o de dados entre o Azure SQL Database, Azure Data Lake Storage Gen2 e o Azure Databricks.</p>"},{"location":"adf-config/#1-acessando-o-azure-data-factory","title":"1. Acessando o Azure Data Factory","text":"<p>Acesse o recurso Azure Data Factory criado por Terraform em seu ambiente Azure:</p> <ol> <li>V\u00e1 para o Portal Azure</li> <li>Na barra de pesquisa, digite \"Data Factory\"</li> <li>Selecione o recurso com o nome provisionado no Terraform.</li> <li>Clique em \"Iniciar o Studio\" (Open Azure Data Factory Studio) para acessar o ambiente de integra\u00e7\u00e3o visual.</li> </ol>"},{"location":"adf-config/#2-configurando-os-linked-services","title":"2. Configurando os Linked Services","text":"<p>Com o Azure Data Factory aberto:</p> <ol> <li>No menu \u00e0 esquerda, clique em \"Gerenciar\" (Manage).</li> <li>Selecione \"Servi\u00e7os vinculados\" (Linked Services).</li> <li>Clique em \"Novo\" para adicionar os servi\u00e7os abaixo:</li> </ol>"},{"location":"adf-config/#a-azure-sql-database","title":"a) Azure SQL Database","text":"<ul> <li>Tipo: Azure SQL Database</li> <li>Inscri\u00e7\u00e3o: Selecione sua subscri\u00e7\u00e3o Azure</li> <li>Banco de dados: Selecione o provisionado via Terraform</li> <li>Autentica\u00e7\u00e3o: Nome de usu\u00e1rio e senha (Admin do SQL Server)</li> </ul>"},{"location":"adf-config/#b-azure-data-lake-storage-gen2","title":"b) Azure Data Lake Storage Gen2","text":"<ul> <li>Tipo: Azure Data Lake Storage Gen2</li> <li>Inscri\u00e7\u00e3o: Selecione sua subscri\u00e7\u00e3o Azure</li> <li>Conta de Armazenamento: Selecione a provisionada no Terraform</li> <li>Autentica\u00e7\u00e3o: Conta de Servi\u00e7o (Managed Identity ou Chave de Conta)</li> </ul>"},{"location":"adf-config/#c-azure-databricks","title":"c) Azure Databricks","text":"<ul> <li>Tipo: Azure Databricks</li> <li>Inscri\u00e7\u00e3o: Selecione sua subscri\u00e7\u00e3o Azure</li> <li>Workspace: Selecione o workspace Databricks criado via Terraform</li> <li>Autentica\u00e7\u00e3o: Token pessoal do Databricks  </li> <li>Caso n\u00e3o tenha um token, consulte a Documenta\u00e7\u00e3o do Azure Databricks</li> <li>Cluster: Selecione o cluster interativo existente (criado anteriormente)</li> </ul>"},{"location":"adf-config/#3-criando-os-conjuntos-de-dados-datasets","title":"3. Criando os Conjuntos de Dados (Datasets)","text":""},{"location":"adf-config/#a-dataset-de-origem-azure-sql-database","title":"a) Dataset de Origem: Azure SQL Database","text":"<p>Crie um novo dataset para conectar-se \u00e0s tabelas do banco de dados SQL hospedado no Azure:</p> <ul> <li>Tipo: Azure SQL Database</li> <li>Linked Service: Selecione o servi\u00e7o criado acima</li> <li>Query/Table: Selecione a tabela desejada ou use par\u00e2metros</li> </ul> <p>Exemplo de par\u00e2metro configurado:</p> <p></p> <p>JSON do Dataset:</p> <p>JSON</p> <pre><code>{\n    \"name\": \"DS_SQL_Parametro\",\n    \"properties\": {\n        \"linkedServiceName\": {\n            \"referenceName\": \"AzureSqlDatabase\",\n            \"type\": \"LinkedServiceReference\"\n        },\n        \"parameters\": {\n            \"tabela_nome\": {\n                \"type\": \"string\"\n            }\n        },\n        \"annotations\": [],\n        \"type\": \"AzureSqlTable\",\n        \"schema\": [],\n        \"typeProperties\": {\n            \"schema\": \"relacional\",\n            \"table\": \"avaliacoes\"\n        }\n    }\n}\n</code></pre>"},{"location":"adf-config/#b-dataset-de-destino-azure-data-lake-storage-gen2-delimited-text","title":"b) Dataset de Destino: Azure Data Lake Storage Gen2 (Delimited Text)","text":"<p>Configure um dataset para exportar os dados em formato <code>.csv</code> para a landing zone no Data Lake:</p> <ul> <li>Tipo: Delimited Text</li> <li>Linked Service: Azure Data Lake Storage Gen2</li> <li>Pasta de destino: landing-zone/</li> <li>Nome do arquivo: pode ser parametrizado (ex: <code>@dataset().nome_arquivo</code>)</li> <li>Delimitador: <code>,</code></li> <li>First Row as Header: true</li> </ul> <p>Exemplo de configura\u00e7\u00e3o:</p> <p></p> <p>JSON do Dataset:</p> <p>JSON</p> <pre><code>{\n    \"name\": \"DS_DEST_CSV\",\n    \"properties\": {\n        \"linkedServiceName\": {\n            \"referenceName\": \"AzureDataLakeStorage\",\n            \"type\": \"LinkedServiceReference\"\n        },\n        \"parameters\": {\n            \"nome_arquivo\": {\n                \"type\": \"string\"\n            }\n        },\n        \"annotations\": [],\n        \"type\": \"DelimitedText\",\n        \"typeProperties\": {\n            \"location\": {\n                \"type\": \"AzureBlobFSLocation\",\n                \"fileSystem\": {\n                    \"value\": \"@concat(\\n  'landing-zone/ecommerce/',\\n  trim(\\n    uriComponent(\\n      trim(\\n        split(trim(dataset().nome_arquivo), '.')[1]\\n      )\\n    )\\n  ),\\n  '.csv'\\n)\",\n                    \"type\": \"Expression\"\n                }\n            },\n            \"columnDelimiter\": \",\",\n            \"escapeChar\": \"\\\\\",\n            \"firstRowAsHeader\": true,\n            \"quoteChar\": \"\\\"\"\n        },\n        \"schema\": []\n    }\n}\n</code></pre> <p>\u26a0\ufe0f Importante: Certifique-se de testar a conex\u00e3o de cada Linked Service e Dataset ap\u00f3s a cria\u00e7\u00e3o.</p> <p>Ir para cira\u00e7\u00e3o da pipeline no Azure Data Factory</p>"},{"location":"adf-pipeline/","title":"Azure Data Factory - Pipeline de Integra\u00e7\u00e3o","text":"<p>Este documento descreve como criar e configurar uma pipeline no Azure Data Factory para orquestrar a movimenta\u00e7\u00e3o e transforma\u00e7\u00e3o de dados entre o Azure SQL Database, Azure Data Lake Storage Gen2 e notebooks do Azure Databricks.</p>"},{"location":"adf-pipeline/#1-set-variable-definindo-as-tabelas","title":"1. Set Variable - Definindo as Tabelas","text":"<p>A primeira atividade da pipeline \u00e9 definir uma vari\u00e1vel do tipo array com os nomes das tabelas que ser\u00e3o copiadas do banco de dados.</p> <ol> <li>Na aba de atividades, arraste a atividade Set Variable para a pipeline.</li> <li>Crie uma nova vari\u00e1vel do tipo Array, por exemplo: <code>tabelas</code>.</li> <li>Configure o valor com os nomes das tabelas, por exemplo:</li> </ol> <p>\ud83d\udcf7 Exemplo de configura\u00e7\u00e3o:</p> <p></p> <p>\ud83d\udcc4 Configura\u00e7\u00e3o do array:</p> <p>@{   \"tabelas\": [     \"clientes\",     \"vendedores\",     \"produtos\",     \"pedidos\",     \"pagamentos\"   ] }</p>"},{"location":"adf-pipeline/#2-foreach-loop-nas-tabelas","title":"2. Foreach - Loop nas Tabelas","text":"<ol> <li>Adicione a atividade Foreach ap\u00f3s o <code>Set Variable</code>.</li> <li>Na aba \"Configura\u00e7\u00f5es\" do Foreach, defina como item sequencial e adicione a seguinte express\u00e3o no campo de itens:</li> </ol> <p>@<code>@variables('tabelas')</code></p> <ol> <li>Dentro do Foreach, adicione uma atividade Copy Data.</li> </ol>"},{"location":"adf-pipeline/#configuracoes-da-atividade-copy-data","title":"Configura\u00e7\u00f5es da Atividade Copy Data:","text":"<ul> <li>Origem: Dataset do Azure SQL Database com suporte a par\u00e2metro de nome de tabela.</li> <li>Destino: Dataset de arquivo delimitado no ADLS (tamb\u00e9m parametrizado).</li> </ul> <p>\ud83d\udcf7 Exemplo de configura\u00e7\u00e3o:</p> <p></p> <p>\ud83d\udcc4 Exemplo de mapeamento din\u00e2mico:</p> <p>@{   \"source\": {     \"type\": \"AzureSqlSource\",     \"sqlReaderQuery\": \"SELECT * FROM @{item()}\"   },   \"sink\": {     \"type\": \"DelimitedTextSink\"   },   \"translator\": {     \"type\": \"TabularTranslator\",     \"typeConversion\": true,     \"typeConversionMode\": \"AllowCompatible\"   } }</p>"},{"location":"adf-pipeline/#3-executando-notebook-bronze-no-databricks","title":"3. Executando Notebook Bronze no Databricks","text":"<ol> <li>Ap\u00f3s o loop, adicione uma nova atividade: Databricks Notebook.</li> <li>Configure com o servi\u00e7o vinculado (Linked Service) do Databricks criado anteriormente.</li> <li>Preencha o campo Path com o caminho do notebook bronze, que deve estar no workspace.</li> </ol> <p>\u2139\ufe0f O caminho do notebook pode ser encontrado conforme instru\u00eddo na documenta\u00e7\u00e3o do Azure Databricks.</p>"},{"location":"adf-pipeline/#4-executando-notebook-silver-no-databricks","title":"4. Executando Notebook Silver no Databricks","text":"<ol> <li>Adicione outra atividade Databricks Notebook.</li> <li>Use o mesmo Linked Service.</li> <li>Preencha com o caminho do notebook silver.</li> </ol>"},{"location":"adf-pipeline/#5-executando-notebook-gold-no-databricks","title":"5. Executando Notebook Gold no Databricks","text":"<ol> <li>Adicione mais uma atividade Databricks Notebook.</li> <li>Configure novamente com o Linked Service e o caminho do notebook gold.</li> </ol>"},{"location":"adf-pipeline/#6-consideracoes-finais","title":"6. Considera\u00e7\u00f5es Finais","text":"<ul> <li>Certifique-se de que o cluster do Databricks est\u00e1 ativo antes da execu\u00e7\u00e3o dos notebooks.</li> <li>O nome das tabelas no array do <code>Set Variable</code> deve existir no banco SQL.</li> <li>Os datasets devem estar parametrizados para receber dinamicamente o nome da tabela e do arquivo.</li> <li>O pipeline pode ser agendado ou executado manualmente.</li> <li>Todas as depend\u00eancias (Linked Services, Datasets, e Notebooks) devem estar previamente criadas.</li> </ul>"},{"location":"arquitetura/","title":"Arquitetura do Projeto","text":""},{"location":"arquitetura/#visao-geral","title":"Vis\u00e3o Geral","text":"<p>Este projeto utiliza uma arquitetura moderna baseada em servi\u00e7os gerenciados da Azure para ingest\u00e3o, processamento, armazenamento e visualiza\u00e7\u00e3o de dados.</p>"},{"location":"arquitetura/#componentes-principais","title":"Componentes Principais","text":"<ul> <li>Azure SQL Database: Armazena dados transacionais.</li> <li>Azure Data Lake Storage Gen2 (ADLS): Armazena dados em camadas (Landing, Bronze, Silver, Gold).</li> <li>Azure Data Factory (ADF): Orquestra os pipelines de ingest\u00e3o e transforma\u00e7\u00e3o.</li> <li>Azure Databricks: Processa os dados em notebooks (ETL).</li> <li>Power BI: Conecta-se ao Silver ou Gold para cria\u00e7\u00e3o de dashboards.</li> </ul>"},{"location":"arquitetura/#diagrama-da-arquitetura","title":"Diagrama da Arquitetura","text":""},{"location":"azdabri/","title":"Utilizando o Azure Databricks","text":"<p>Este guia descreve os passos necess\u00e1rios para acessar o workspace do Azure Databricks criado via Terraform, importar notebooks, configurar um cluster de computa\u00e7\u00e3o e gerar um token de acesso.</p>"},{"location":"azdabri/#1-acessando-o-workspace-e-importando-notebooks","title":"1. Acessando o Workspace e Importando Notebooks","text":"<p>Ap\u00f3s a execu\u00e7\u00e3o do Terraform, o recurso Azure Databricks estar\u00e1 provisionado. Caso voc\u00ea n\u00e3o tenha feito o provisionamento dos resursos, acesse a documenta\u00e7\u00e3o do Terraform do projeto. Para acessar o workspace:</p> <ol> <li>Acesse o portal do Azure.</li> <li>Pesquise por \"Azure Databricks\" e clique no recurso criado.</li> <li>Clique em \"Launch Workspace\" para abrir a interface do Databricks.</li> <li>No menu lateral esquerdo, v\u00e1 at\u00e9 a se\u00e7\u00e3o \"Workspace\".</li> <li>Clique com o bot\u00e3o direito em seu usu\u00e1rio ou pasta principal e selecione \"Create &gt; Folder\".</li> <li> <p>Nomeie a pasta como <code>notebooks</code>.</p> </li> <li> <p>Dentro da pasta <code>notebooks</code>, clique em \"Import\".</p> </li> <li>Selecione os arquivos localizados na pasta do projeto: <code>src/notebooks/</code></li> <li>Importe todos os arquivos <code>.ipynb</code> necess\u00e1rios para o seu trabalho.</li> </ol>"},{"location":"azdabri/#2-criando-um-cluster-de-computacao","title":"2. Criando um Cluster de Computa\u00e7\u00e3o","text":"<p>Para executar os notebooks, ser\u00e1 necess\u00e1rio configurar um cluster Databricks.</p> <ol> <li>No menu esquerdo, clique em \"Compute\".</li> <li>Clique no bot\u00e3o \"Create Cluster\".</li> <li> <p>Preencha as informa\u00e7\u00f5es conforme abaixo:</p> </li> <li> <p>Cluster name: Nome de sua escolha (ex: <code>cluster-projeto</code>)</p> </li> <li>Databricks Runtime Version: <code>10.4 LTS (Scala 2.12, Spark 3.2.1)</code></li> <li>Node type: <code>Standard_D4s_v3</code></li> <li> <p>Autopilot Options: Pode deixar as op\u00e7\u00f5es padr\u00e3o ou configurar conforme necessidade.</p> </li> <li> <p>Clique em \"Create Cluster\".</p> </li> </ol> <p></p>"},{"location":"azdabri/#3-gerando-um-token-de-acesso-para-o-azure-data-factory","title":"3. Gerando um Token de Acesso (para o Azure Data Factory)","text":"<p>Para integra\u00e7\u00e3o com outros servi\u00e7os, como o Azure Data Factory, \u00e9 necess\u00e1rio um token de acesso do Databricks.</p> <ol> <li>No canto superior direito da interface do Databricks, clique no \u00edcone de usu\u00e1rio e depois em \"User Settings\".</li> <li>V\u00e1 at\u00e9 a aba \"Developer\" ou \"Access Tokens\".</li> <li>Clique em \"Generate New Token\".</li> <li>Defina o tempo de expira\u00e7\u00e3o desejado.</li> <li>Clique em \"Generate\" e copie o token gerado.</li> </ol> <p>\u26a0\ufe0f Aten\u00e7\u00e3o: o token n\u00e3o poder\u00e1 ser recuperado novamente, portanto salve-o com seguran\u00e7a. Voc\u00ea ir\u00e1 utiliz\u00e1-lo posteriormente na configura\u00e7\u00e3o do Azure Data Factory.</p>"},{"location":"azdabri/#recursos-relacionados","title":"Recursos Relacionados","text":"<ul> <li>Pasta com notebooks: <code>src/notebooks/</code></li> <li>Imagem de refer\u00eancia do cluster: <code>assets/foto-compute.png</code></li> </ul>"},{"location":"azdabri/#requisitos","title":"Requisitos","text":"<ul> <li>O Azure Databricks deve estar provisionado via Terraform.</li> <li>Voc\u00ea precisa de permiss\u00f5es para acessar o recurso e criar notebooks/cluster.</li> </ul>"},{"location":"faker/","title":"Gerando Dados Falsos com Faker","text":"<p>Nesta etapa, utilizamos a biblioteca Faker para popular automaticamente o banco de dados com dados sint\u00e9ticos realistas. Isso facilita o teste do sistema com um volume significativo de dados.</p>"},{"location":"faker/#arquivos-relacionados","title":"Arquivos Relacionados","text":"<ul> <li><code>faker_data.py</code> \u2014 Script principal de gera\u00e7\u00e3o e inser\u00e7\u00e3o de dados.</li> <li><code>.env.example</code> \u2014 Modelo de vari\u00e1veis de ambiente necess\u00e1rias.</li> <li><code>teste_conexao.py</code> \u2014 Script para testar a conex\u00e3o com o banco.</li> </ul>"},{"location":"faker/#como-funciona","title":"Como Funciona","text":"<p>O script <code>faker_data.py</code> utiliza:</p> <ul> <li>Faker para gerar dados como nomes, emails, endere\u00e7os, datas etc.</li> <li>pyodbc para conectar-se ao SQL Server hospedado na Azure.</li> <li>dotenv para carregar vari\u00e1veis de ambiente de um arquivo <code>.env</code>.</li> </ul> <p>Ele gera dados para diversas tabelas do schema <code>relacional</code>, como:</p> <ul> <li><code>clientes</code>, <code>vendedores</code>, <code>produtos</code>, <code>pedidos</code>, <code>pagamentos</code>, <code>entregas</code> e mais.</li> <li>Categorias e formas de pagamento s\u00e3o fixas.</li> <li>A inser\u00e7\u00e3o \u00e9 feita em lote, e a execu\u00e7\u00e3o s\u00f3 come\u00e7a se todas as tabelas existirem.</li> </ul>"},{"location":"faker/#antes-de-executar","title":"Antes de Executar","text":""},{"location":"faker/#1-criar-o-banco-de-dados-se-necessario","title":"1. Criar o Banco de Dados (se necess\u00e1rio)","text":"<p>Se ainda n\u00e3o houver um Azure SQL Database criado, consulte a documenta\u00e7\u00e3o de provisionamento com Terraform:</p> <p>Provisionamento com Terraform</p>"},{"location":"faker/#2-instalar-o-driver-odbc-para-sql-server","title":"2. Instalar o Driver ODBC para SQL Server","text":"<p>\u00c9 necess\u00e1rio ter o driver ODBC instalado no sistema.</p> <ul> <li> <p>Windows: Download do ODBC Driver</p> </li> <li> <p>Linux/Mac:   Consulte o guia oficial para instru\u00e7\u00f5es espec\u00edficas.</p> </li> </ul>"},{"location":"faker/#3-configurar-as-variaveis-de-ambiente","title":"3. Configurar as Vari\u00e1veis de Ambiente","text":"<p>Crie um arquivo <code>.env</code> na raiz do projeto com base no <code>.env.example</code>:</p> <pre><code>DB_SERVER=seu_servidor.database.windows.net  \nDB_DATABASE=nome_do_banco  \nDB_USERNAME=seu_usuario  \nDB_PASSWORD=sua_senha  \nDB_DRIVER=ODBC Driver 18 for SQL Server  \n</code></pre>"},{"location":"faker/#4-garantir-que-as-tabelas-estao-criadas","title":"4. Garantir que as Tabelas Est\u00e3o Criadas","text":"<p>Antes de rodar o script, certifique-se de que todas as tabelas do banco j\u00e1 foram criadas.</p> <p>Consulte:</p> <ul> <li>Scripts SQL fornecidos pelo projeto, ou  </li> <li>A documenta\u00e7\u00e3o de banco de dados: Documenta\u00e7\u00e3o do Banco</li> </ul>"},{"location":"faker/#5-testar-a-conexao-com-o-banco","title":"5. Testar a Conex\u00e3o com o Banco","text":"<p>Execute o script de teste:</p> <pre><code>python teste_conexao.py  \n</code></pre>"},{"location":"faker/#6-executar-o-script-faker","title":"6. Executar o Script Faker","text":"<p>Se a conex\u00e3o estiver correta, execute:</p> <pre><code>python faker_data.py  \n</code></pre>"},{"location":"faker/#observacoes-importantes","title":"\u26a0\ufe0f Observa\u00e7\u00f5es Importantes","text":"<ul> <li>O script valida a exist\u00eancia das tabelas antes de iniciar.</li> <li>Dados s\u00e3o sempre os mesmos, pois o script usa seeds fixas (<code>Faker.seed(42)</code>, <code>random.seed(42)</code>).</li> <li>N\u00e3o h\u00e1 limpeza de dados anterior \u2014 evite rodar m\u00faltiplas vezes se o banco j\u00e1 contiver registros.</li> <li>Se alguma tabela estiver faltando, a execu\u00e7\u00e3o ser\u00e1 interrompida com mensagem de erro.</li> <li>Para garantir que tudo est\u00e1 em ordem, verifique a estrutura do banco de dados antes de usar o Faker.</li> </ul>"},{"location":"iac/","title":"Infraestrutura como C\u00f3digo (IaC) com Terraform","text":"<p>Este projeto utiliza Terraform para provisionar a infraestrutura na Azure, adotando uma abordagem modular. Cada m\u00f3dulo est\u00e1 isolado em sua pr\u00f3pria pasta e deve ser aplicado separadamente.</p>"},{"location":"iac/#estrutura-dos-modulos","title":"Estrutura dos M\u00f3dulos","text":"<p>Cada pasta representa um m\u00f3dulo de infraestrutura:</p> <ul> <li><code>resource_group/</code> \u2014 Grupo de recursos no Azure  </li> <li><code>sql_server/</code> \u2014 Azure SQL Database</li> <li><code>adls/</code> \u2014 Azure Data Lake Storage  </li> <li><code>az_databricks/</code> \u2014 Azure Databricks  </li> <li><code>adf/</code> \u2014 Azure Data Factory  </li> </ul>"},{"location":"iac/#como-executar","title":"Como executar","text":""},{"location":"iac/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<ul> <li>Azure CLI  </li> <li>Terraform  </li> <li>Conta Azure com permiss\u00f5es adequadas  </li> </ul>"},{"location":"iac/#passos-para-provisionar-cada-modulo","title":"Passos para provisionar cada m\u00f3dulo","text":"<p>Para provisionar os recursos, voc\u00ea deve entrar em cada pasta de m\u00f3dulo e executar os comandos do Terraform individualmente. Por exemplo, para o m\u00f3dulo <code>resource_group</code>:</p> <pre><code>cd resource_group  \nterraform init  \nterraform plan -var-file=\"../terraform.tfvars\"  \nterraform apply -var-file=\"../terraform.tfvars\"  \n</code></pre> <p>Repita esses passos para cada m\u00f3dulo (<code>sql_server</code>, <code>adls</code>, <code>az_databricks</code>, <code>adf</code>), sempre navegando para a respectiva pasta antes de executar os comandos.</p>"},{"location":"iac/#comandos-gerais-para-cada-modulo","title":"Comandos gerais para cada m\u00f3dulo","text":"<pre><code>terraform init  \nterraform plan -var-file=\"../terraform.tfvars\"  \nterraform apply -var-file=\"../terraform.tfvars\"  \n</code></pre>"},{"location":"iac/#exemplos-do-arquivo-terraformtfvars","title":"Exemplos do arquivo terraform.tfvars","text":"<p>Aqui est\u00e3o exemplos fict\u00edcios de valores para cada m\u00f3dulo no arquivo <code>terraform.tfvars</code>:</p>"},{"location":"iac/#resource_group","title":"resource_group","text":"<pre><code>resource_group_name = \"rg-exemplo-projeto\"  \nlocation            = \"eastus\"  \n</code></pre>"},{"location":"iac/#sql_server","title":"sql_server","text":"<pre><code>subscription_id     = \"12345678-1234-1234-1234-123456789abc\"  \nresource_group_name = \"rg-exemplo-projeto\"  \nusuario_admin       = \"admin_exemplo\"  \npassword            = \"SenhaForte!2025\"  \n</code></pre>"},{"location":"iac/#az_databricks","title":"az_databricks","text":"<pre><code>azure_client_id     = \"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee\"  \nazure_client_secret = \"AaBbCcDdEeFfGgHhIiJjKkLlMmNnOoPpQqRrSsTtUuVv\"  \nazure_tenant_id     = \"ffffffff-1111-2222-3333-444444444444\"  \nworkspace_name      = \"ws-exemplo-databricks\"  \nsubscription_id     = \"12345678-1234-1234-1234-123456789abc\"  \nresource_group_name = \"rg-exemplo-projeto\"  \n</code></pre>"},{"location":"iac/#adls","title":"adls","text":"<pre><code>subscription_id     = \"12345678-1234-1234-1234-123456789abc\"  \nresource_group_name = \"rg-exemplo-projeto\"  \n</code></pre>"},{"location":"iac/#adf","title":"adf","text":"<pre><code>resource_group_name = \"rg-exemplo-projeto\"  \ndata_factory_name   = \"adf-exemplo-projeto\"  \n</code></pre>"},{"location":"iac/#autenticacao-no-azure","title":"Autentica\u00e7\u00e3o no Azure","text":"<p>Antes de iniciar o provisionamento, \u00e9 necess\u00e1rio autenticar na sua conta Azure usando o Azure CLI:</p> <pre><code>az login  \n</code></pre> <p>Esse comando abrir\u00e1 uma janela no navegador para login. Ap\u00f3s logado, voc\u00ea poder\u00e1 executar comandos <code>az</code> para consultar os recursos e configurar vari\u00e1veis no <code>terraform.tfvars</code>.</p>"},{"location":"iac/#como-obter-os-valores-para-o-terraformtfvars","title":"Como obter os valores para o terraform.tfvars","text":"<p>Use os comandos abaixo para descobrir os valores usados nas vari\u00e1veis dos m\u00f3dulos:</p>"},{"location":"iac/#subscription_id","title":"subscription_id","text":"<pre><code>az account show --query id -o tsv  \n</code></pre>"},{"location":"iac/#resource_group_name","title":"resource_group_name","text":"<p><pre><code>az group list --query \"[].name\" -o tsv  \n</code></pre> recomendado utilizar o mesmo grupo de recursos para todos os servi\u00e7os provisionados.</p>"},{"location":"iac/#location","title":"location","text":"<pre><code>az account list-locations --query \"[].{Region:name}\" -o table  \n</code></pre>"},{"location":"iac/#azure_client_id-azure_client_secret-e-azure_tenant_id","title":"azure_client_id, azure_client_secret e azure_tenant_id","text":"<p>Para criar um novo Service Principal:</p> <pre><code>az ad sp create-for-rbac --name \"sp-nome-exemplo\" --role Contributor --scopes /subscriptions/&lt;subscription_id&gt;  \n</code></pre> <p>O retorno incluir\u00e1:</p> <ul> <li><code>appId</code> \u2192 <code>azure_client_id</code> </li> <li><code>password</code> \u2192 <code>azure_client_secret</code> </li> <li><code>tenant</code> \u2192 <code>azure_tenant_id</code> </li> </ul> <p>\u26a0\ufe0f Importante: guarde esses dados com seguran\u00e7a e nunca versiona esse arquivo no Git (o projeto ja possui um .gitigore com esse arquivo em cada pasta).</p>"},{"location":"iac/#consideracoes-finais","title":"Considera\u00e7\u00f5es finais","text":"<ul> <li>O arquivo <code>terraform.tfvars</code> na raiz deve conter as vari\u00e1veis de configura\u00e7\u00e3o utilizadas por todos os m\u00f3dulos.  </li> <li>Para destruir recursos, entre em cada m\u00f3dulo e rode:</li> </ul> <pre><code>terraform destroy -var-file=\"../terraform.tfvars\"  \n</code></pre>"},{"location":"notebooks/","title":"Documenta\u00e7\u00e3o dos Notebooks Databricks","text":"<p>Esta documenta\u00e7\u00e3o apresenta os notebooks que fazem o processo de ingest\u00e3o e tratamento dos dados em tr\u00eas camadas: Bronze, Silver e Gold .</p>"},{"location":"notebooks/#notebook-bronze","title":"Notebook Bronze","text":""},{"location":"notebooks/#objetivo","title":"Objetivo","text":"<p>Ingerir os arquivos CSV da landing zone no armazenamento Azure Data Lake (ADLS), montar os containers, carregar os dados em DataFrames Spark, aplicar algumas transforma\u00e7\u00f5es b\u00e1sicas e salvar no formato Delta na camada Bronze.</p>"},{"location":"notebooks/#configuracoes-e-montagem","title":"Configura\u00e7\u00f5es e Montagem","text":"<pre><code>storageAccountName = \"\"\nstorageAccountAccessKey = \"\"\nsasToken = \"\"\n\ndef mount_adls(blobContainerName):\n    try:\n        dbutils.fs.mount(\n            source = \"wasbs://{}@{}.blob.core.windows.net\".format(blobContainerName, storageAccountName),\n            mount_point = f\"/mnt/{storageAccountName}/{blobContainerName}\",\n            extra_configs = {'fs.azure.sas.' + blobContainerName + '.' + storageAccountName + '.blob.core.windows.net': sasToken}\n        )\n        print(\"OK!\")\n    except Exception as e:\n        print(\"Falha\", e)\n\nmount_adls('landing-zone')\nmount_adls('bronze')\nmount_adls('silver')\nmount_adls('gold')\n</code></pre>"},{"location":"notebooks/#ingestao-dos-dados-csv","title":"Ingest\u00e3o dos Dados CSV","text":"<p>Leitura dos arquivos CSV da landing zone para os DataFrames Spark e defini\u00e7\u00e3o dos nomes das colunas.</p> <p>Exemplo para o arquivo vendedores:</p> <pre><code>df_vendedores_raw = spark.read.option(\"header\", \"false\").csv(f\"/mnt/{storageAccountName}/landing-zone/ecommerce/vendedores.csv\")\ncolunas_vendedores = [\"id\", \"nome\", \"email\", \"telefone\", \"data_cadastro\"]\ndf_vendedores = df_vendedores_raw.toDF(*colunas_vendedores)\n</code></pre> <p>Processo similar para os demais arquivos:</p> <ul> <li>clientes.csv</li> <li>categorias.csv</li> <li>estoque.csv</li> <li>pagamentos.csv</li> <li>entregas.csv</li> <li>avaliacoes.csv</li> <li>pedidos.csv</li> <li>enderecos_cliente.csv</li> <li>transportadoras.csv</li> <li>formas_pagamento.csv</li> <li>itens_pedido.csv</li> <li>produtos.csv</li> </ul>"},{"location":"notebooks/#enriquecimento-dos-dataframes","title":"Enriquecimento dos DataFrames","text":"<p>Adi\u00e7\u00e3o das colunas <code>data_hora_bronze</code> (timestamp da ingest\u00e3o) e <code>nome_arquivo</code> para rastreamento.</p> <pre><code>from pyspark.sql.functions import current_timestamp, lit\n\ndf_vendedores = df_vendedores.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"vendedores.csv\"))\n# Mesmo para os demais DataFrames...\n</code></pre>"},{"location":"notebooks/#sanitizacao-dos-nomes-das-colunas","title":"Sanitiza\u00e7\u00e3o dos nomes das colunas","text":"<p>Fun\u00e7\u00e3o para padronizar os nomes das colunas:</p> <pre><code>def sanitize_columns(df):\n    for col_name in df.columns:\n        new_name = (\n            col_name.strip()\n            .lower()\n            .replace(\" \", \"_\")\n            .replace(\"(\", \"\")\n            .replace(\")\", \"\")\n            .replace(\"-\", \"_\")\n            .replace(\",\", \"\")\n            .replace(\";\", \"\")\n            .replace(\"{\", \"\")\n            .replace(\"}\", \"\")\n            .replace(\"=\", \"\")\n            .replace(\"\\n\", \"\")\n            .replace(\"\\t\", \"\")\n        )\n        df = df.withColumnRenamed(col_name, new_name)\n    return df\n</code></pre>"},{"location":"notebooks/#salvamento-dos-dados-na-camada-bronze","title":"Salvamento dos dados na camada Bronze","text":"<pre><code>dfs = [\n    (df_vendedores, \"vendedores\"),\n    (df_clientes, \"clientes\"),\n    (df_categorias, \"categorias\"),\n    (df_estoque, \"estoque\"),\n    (df_pagamentos, \"pagamentos\"),\n    (df_entregas, \"entregas\"),\n    (df_avaliacoes, \"avaliacoes\"),\n    (df_formas_pagamento, \"formas_pagamento\"),\n    (df_transportadoras, \"transportadoras\"),\n    (df_pedidos, \"pedidos\"),\n    (df_enderecos_cliente, \"enderecos_cliente\"),\n    (df_itens_pedido, \"itens_pedido\"),\n    (df_produtos, \"produtos\"),\n]\n\nfor df, name in dfs:\n    df_sanitized = sanitize_columns(df)\n    path = f\"/mnt/{storageAccountName}/bronze/ecommerce/{name}\"\n    df_sanitized.write.format(\"delta\").save(path)\n</code></pre>"},{"location":"notebooks/#notebook-silver","title":"Notebook Silver","text":""},{"location":"notebooks/#objetivo_1","title":"Objetivo","text":"<p>Ler os dados da camada Bronze, aplicar tratamentos e renomea\u00e7\u00f5es, adicionar metadados, e salvar os dados processados na camada Silver.</p>"},{"location":"notebooks/#montagem-dos-containers-mesma-funcao-do-notebook-bronze","title":"Montagem dos containers (mesma fun\u00e7\u00e3o do notebook Bronze)","text":"<pre><code>storageAccountName = \"\"\nstorageAccountAccessKey = \"\"\nsasToken= \"\"\n\ndef mount_adls(blobContainerName):\n    try:\n        dbutils.fs.mount(\n            source = \"wasbs://{}@{}.blob.core.windows.net\".format(blobContainerName, storageAccountName),\n            mount_point = f\"/mnt/{storageAccountName}/{blobContainerName}\",\n            extra_configs = {'fs.azure.sas.' + blobContainerName + '.' + storageAccountName + '.blob.core.windows.net': sasToken}\n        )\n        print(\"OK!\")\n    except Exception as e:\n        print(\"Falha\", e)\n</code></pre>"},{"location":"notebooks/#leitura-dos-dados-bronze-em-formato-delta","title":"Leitura dos dados Bronze em formato Delta","text":"<pre><code>df_avaliacoes          = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/avaliacoes\")\ndf_categorias          = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/categorias\")\ndf_clientes            = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/clientes\")\ndf_enderecos_cliente   = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/enderecos_cliente\")\ndf_entregas            = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/entregas\")\ndf_estoque             = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/estoque\")\ndf_formas_pagamento    = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/formas_pagamento\")\ndf_itens_pedidos       = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/itens_pedido\")\ndf_pagamentos          = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/pagamentos\")\ndf_pedidos             = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/pedidos\")\ndf_produtos            = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/produtos\")\ndf_transportadoras     = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/transportadoras\")\ndf_vendedores          = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/ecommerce/vendedores\")\n</code></pre>"},{"location":"notebooks/#adicao-de-colunas-de-metadados-para-silver","title":"Adi\u00e7\u00e3o de colunas de metadados para Silver","text":"<pre><code>from pyspark.sql.functions import current_timestamp, lit\n\ndf_avaliacoes = df_avaliacoes.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"avaliacoes\"))\n# Repetir para os demais DataFrames\n</code></pre>"},{"location":"notebooks/#renomeacao-das-colunas-para-padrao-maiusculo-e-tratamento-de-sufixos","title":"Renomea\u00e7\u00e3o das colunas para padr\u00e3o mai\u00fasculo e tratamento de sufixos","text":"<p>Fun\u00e7\u00e3o para renomear as colunas:</p> <pre><code>from pyspark.sql.functions import lit, current_timestamp\n\ndef renomear_colunas(diretorio):\n    df = spark.read.format('delta').load(diretorio)\n    tabela = diretorio.split('/')[-2]\n\n    novos_nomes = {}\n\n    for coluna in df.columns:\n        novo_nome = coluna.upper()\n\n        if novo_nome.endswith(\"_ID\"):\n            prefixo = novo_nome[:-3]\n            novo_nome = f\"CODIGO_{prefixo}\"\n        else:\n            novo_nome = novo_nome.replace(\"ID\", \"CODIGO\")\n\n        novos_nomes[coluna] = novo_nome\n\n    for antigo, novo in novos_nomes.items():\n        df = df.withColumnRenamed(antigo, novo)\n\n    for col_drop in [\"DATA_HORA_BRONZE\", \"NOME_ARQUIVO\"]:\n        if col_drop in df.columns:\n            df = df.drop(col_drop)\n\n    df = df.withColumn(\"NOME_ARQUIVO_BRONZE\", lit(tabela))\n    df = df.withColumn(\"DATA_ARQUIVO_SILVER\", current_timestamp())\n\n    df.write.format('delta').mode(\"overwrite\").save(f\"/mnt/{storageAccountName}/silver/ecommerce/{tabela}\")\n\ndef renomear_arquivos_delta(diretorio):\n    arquivos = dbutils.fs.ls(diretorio)\n    for arquivo in arquivos:\n        renomear_colunas(arquivo.path)\n\ndiretorio = f'/mnt/{storageAccountName}/bronze/ecommerce'\nrenomear_arquivos_delta(diretorio)\n</code></pre>"},{"location":"powerbi/","title":"Documenta\u00e7\u00e3o Power BI - Projeto Ecommerce","text":"<p>Este documento descreve os indicadores, m\u00e9tricas e fontes de dados utilizadas no dashboard Power BI constru\u00eddo sobre a camada Gold do Data Lake.</p>"},{"location":"powerbi/#objetivo","title":"Objetivo","text":"<p>Consumir os dados processados da camada Gold no Power BI para an\u00e1lise de vendas, desempenho por produto, comportamento do cliente e efici\u00eancia da opera\u00e7\u00e3o log\u00edstica e financeira.</p>"},{"location":"powerbi/#conexao-com-os-dados","title":"Conex\u00e3o com os Dados","text":"<p>Os dados est\u00e3o armazenados em formato Delta no Azure Data Lake Storage (ADLS) Gen2, organizados nas seguintes tabelas dimensionais e fato:</p> Tabela Caminho no Data Lake Fato Vendas fato_vendas Clientes dim_clientes Entregas dim_entregas Formas de Pagamento dim_formas_pagamento Produtos dim_produtos Tempo dim_tempo Vendedores dim_vendedores"},{"location":"powerbi/#kpis-utilizados","title":"KPIs Utilizados","text":"<p>As principais m\u00e9tricas de desempenho utilizadas no relat\u00f3rio incluem:</p> <ul> <li> <p>Clientes \u00danicos <code>Clientes \u00danicos = DISTINCTCOUNT(fato_vendas[cliente_sk])</code></p> </li> <li> <p>Quantidade Total de Itens Vendidos <code>Quantidade Total = SUM(fato_vendas[quantidade])</code></p> </li> <li> <p>Receita Total <code>Receita Total = SUM(fato_vendas[valor_total])</code></p> </li> <li> <p>Ticket M\u00e9dio <code>Ticket M\u00e9dio = DIVIDE([Receita Total], COUNT(fato_vendas[id]))</code></p> </li> </ul>"},{"location":"powerbi/#metricas-por-dimensao","title":"M\u00e9tricas por Dimens\u00e3o","text":"<p>Essas m\u00e9tricas possibilitam cortes estrat\u00e9gicos por diferentes dimens\u00f5es de neg\u00f3cio:</p> <ul> <li> <p>Receita Total por Categoria de Produto   Dimens\u00e3o: <code>dim_produtos[categoria]</code>   Medida: <code>Receita Total</code></p> </li> <li> <p>Receita Total por Ano   Dimens\u00e3o: <code>dim_tempo[data]</code>   Medida: <code>Receita Total</code></p> </li> </ul>"},{"location":"powerbi/#consideracoes-finais","title":"Considera\u00e7\u00f5es Finais","text":"<p>A camada Gold fornece dados tratados, limpos e modelados prontos para consumo anal\u00edtico. A conex\u00e3o no Power BI foi feita utilizando o conector Azure Data Lake Gen2 com autentica\u00e7\u00e3o baseada em chave ou OAuth, dependendo do ambiente.</p> <p>Para visualizar o dashboard, clique aqui.</p>"}]}