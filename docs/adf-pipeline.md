# Azure Data Factory - Pipeline de Integra√ß√£o

Este documento descreve como criar e configurar uma **pipeline no Azure Data Factory** para orquestrar a movimenta√ß√£o e transforma√ß√£o de dados entre o Azure SQL Database, Azure Data Lake Storage Gen2 e notebooks do Azure Databricks.

---

## 1. Set Variable - Definindo as Tabelas

A primeira atividade da pipeline √© definir uma **vari√°vel do tipo array** com os nomes das tabelas que ser√£o copiadas do banco de dados.

1. Na aba de atividades, arraste a atividade **Set Variable** para a pipeline.
2. Crie uma nova vari√°vel do tipo **Array**, por exemplo: `tabelas`.
3. Configure o valor com os nomes das tabelas, por exemplo:

üì∑ Exemplo de configura√ß√£o:

![Set Variable com array de tabelas](/assets/set-variable-pipeline.png)

üìÑ **Configura√ß√£o do array:**

@{
  "tabelas": [
    "clientes",
    "vendedores",
    "produtos",
    "pedidos",
    "pagamentos"
  ]
}

---

## 2. Foreach - Loop nas Tabelas

1. Adicione a atividade **Foreach** ap√≥s o `Set Variable`.
2. Na aba "Configura√ß√µes" do Foreach, defina como **item sequencial** e adicione a seguinte express√£o no campo de itens:

@`@variables('tabelas')`

3. Dentro do Foreach, adicione uma atividade **Copy Data**.

### Configura√ß√µes da Atividade Copy Data:

- **Origem:** Dataset do Azure SQL Database com suporte a par√¢metro de nome de tabela.
- **Destino:** Dataset de arquivo delimitado no ADLS (tamb√©m parametrizado).

üì∑ Exemplo de configura√ß√£o:

![Copy Data na pipeline](/assets/foreach-copy.png)

üìÑ **Exemplo de mapeamento din√¢mico:**

@{
  "source": {
    "type": "AzureSqlSource",
    "sqlReaderQuery": "SELECT * FROM @{item()}"
  },
  "sink": {
    "type": "DelimitedTextSink"
  },
  "translator": {
    "type": "TabularTranslator",
    "typeConversion": true,
    "typeConversionMode": "AllowCompatible"
  }
}

---

## 3. Executando Notebook Bronze no Databricks

1. Ap√≥s o loop, adicione uma nova atividade: **Databricks Notebook**.
2. Configure com o servi√ßo vinculado (Linked Service) do Databricks criado anteriormente.
3. Preencha o campo **Path** com o caminho do notebook **bronze**, que deve estar no workspace.

> ‚ÑπÔ∏è O caminho do notebook pode ser encontrado conforme instru√≠do na [documenta√ß√£o do Azure Databricks](azdabri.md).

---

## 4. Executando Notebook Silver no Databricks

1. Adicione outra atividade **Databricks Notebook**.
2. Use o mesmo Linked Service.
3. Preencha com o caminho do notebook **silver**.

---

## 5. Executando Notebook Gold no Databricks

1. Adicione mais uma atividade **Databricks Notebook**.
2. Configure novamente com o Linked Service e o caminho do notebook **gold**.

---

## 6. Considera√ß√µes Finais

- Certifique-se de que o cluster do Databricks est√° ativo antes da execu√ß√£o dos notebooks.
- O nome das tabelas no array do `Set Variable` deve existir no banco SQL.
- Os datasets devem estar parametrizados para receber dinamicamente o nome da tabela e do arquivo.
- O pipeline pode ser agendado ou executado manualmente.
- Todas as depend√™ncias (Linked Services, Datasets, e Notebooks) devem estar previamente criadas.

Voc√™ pode encontrar todos os JSONs das atividades e pipeline em `src/adf/pipeline`. Lembre-se, os valores das vari√°veis e caminhos podem alterar no seu ambiente.

---
